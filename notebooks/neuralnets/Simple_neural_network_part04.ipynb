{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Neural network\n",
    "## Part 4: Vectorization\n",
    "\n",
    "This tutorial is part 3 of the previous tutorials on neural networks (TODO: url). While the previous tutorials described very simple single layer regression and classification models, this tutorial will describe a 2-class classification neural network with 1 input dimension, and a non-linear hidden layer with 2 dimensions. While we didn't add the bias parameters to the previous 2 models, we will add them to this model. The network of this model is shown in the following figure:\n",
    "\n",
    "![Image of the logistic model](https://dl.dropboxusercontent.com/u/8938051/Blog_images/SimpleANN03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D plots\n",
    "from matplotlib import cm # Colormaps\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset \n",
    "\n",
    "In this example the target classes $t$ will be generated from 2 class distributions: blue ($t=1$) and red ($t=0$). Where the red class is a circular distribution that surrounds the distribution of the blue class. This results in a 2D dataset that is not linearly seperable. The model from part 2 won't be able to classify both classes correctly since it can learn only linear seperators. By adding a hidden layer the model will be able to train a non-linear seperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and generate the samples\n",
    "nb_of_samples_per_class = 50  # The number of sample in each class\n",
    "\n",
    "# Generate blue samples\n",
    "blue_mean = [0,0]  # The mean of the blue class\n",
    "blue_std_dev = 0.3  # standard deviation of blue class\n",
    "x_blue = np.random.randn(nb_of_samples_per_class, 2) * blue_std_dev + blue_mean\n",
    "\n",
    "# Generate red samples as circle around blue samples\n",
    "red_radius_mean = 1.3  # mean of the radius\n",
    "red_radius_std_dev = 0.2  # standard deviation of the radius\n",
    "red_rand_radius = np.random.randn(nb_of_samples_per_class) * red_radius_std_dev + red_radius_mean\n",
    "red_rand_angle = 2 * np.pi * np.random.rand(nb_of_samples_per_class);\n",
    "x_red = np.asmatrix([red_rand_radius * np.cos(red_rand_angle), \n",
    "                     red_rand_radius * np.sin(red_rand_angle)]).T\n",
    "\n",
    "# Define target vectors for blue and red\n",
    "t_blue_vector = np.asarray([1, 0])\n",
    "t_red_vector = np.asarray([0, 1])\n",
    "# Define the full target matrix for each class\n",
    "t_blue = np.tile(t_blue_vector, (nb_of_samples_per_class, 1))\n",
    "t_red = np.tile(t_red_vector, (nb_of_samples_per_class, 1))\n",
    "\n",
    "# Merge samples in set of input variables x, and corresponding set of\n",
    "# output variables t\n",
    "X = np.vstack((x_blue, x_red))\n",
    "T = np.vstack((t_blue, t_red))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot both classes on the x1, x2 plane\n",
    "plt.plot(x_red[:,0], x_red[:,1], 'ro', label='class red')\n",
    "plt.plot(x_blue[:,0], x_blue[:,1], 'bo', label='class blue')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('$x_1$', fontsize=15)\n",
    "plt.ylabel('$x_2$', fontsize=15)\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "plt.title('red vs blue classes in the input space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of backpropagation\n",
    "\n",
    "### 1. Vectorization of the forward step\n",
    "\n",
    "#### Compute activations of hidden layer\n",
    "The $n$ input samples with $2$ variables each are given as a $n \\times 2$ matrix $X$:\n",
    "\n",
    "$$X =\n",
    "\\begin{bmatrix} \n",
    "x_{11} & x_{12} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_{n1} & x_{n2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Where $x_{ij}$ is the value of the $j$-th variable of the $i$-th input sample. These inputs are projected onto the 3 dimensions of the hidden layer $H$ by the weight matrix $W$ ($w_{hij}$ is the weight of the connection between input variable $i$ and hidden neuron activation $j$) and bias vector $b$ :\n",
    "\n",
    "$$\\begin{align}\n",
    "W_h =\n",
    "\\begin{bmatrix} \n",
    "w_{h11} & w_{h12} & w_{h13} \\\\\n",
    "w_{h21} & w_{h22} & w_{h23}\n",
    "\\end{bmatrix}\n",
    "&& b_h = \n",
    "\\begin{bmatrix} \n",
    "b_{h1} & b_{h2} & b_{h3}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "following computation: \n",
    "\n",
    "$$H = \\sigma(X \\cdot W_h + b_h) = \\frac{1}{1+e^{-(X \\cdot W_h + b_h)}} = \\begin{bmatrix} \n",
    "h_{11} & h_{12} & h_{13} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "h_{n1} & h_{n2} & h_{n3}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "With $\\sigma$ the logistic function, and with $H$ resulting in a $n \\times 3$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute activations of output \n",
    "\n",
    "To compute the output activations the hidden layer activations can be projected onto the 2 dimensional output layer. This is done by the $3 \\times 2$ weight matrix $W_o$ ($w_{oij}$ is the weight of the connection between hidden layer neuron $i$ and output activation $j$) and $2 \\times 1$ bias vector $b_o$ :\n",
    "\n",
    "$$\\begin{align}\n",
    "W_o =\n",
    "\\begin{bmatrix} \n",
    "w_{o11} & w_{o12} \\\\\n",
    "w_{o21} & w_{o22} \\\\\n",
    "w_{o31} & w_{o32}\n",
    "\\end{bmatrix}\n",
    "&& b_o = \n",
    "\\begin{bmatrix} \n",
    "b_{o1} & b_{o2}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "following computation: \n",
    "\n",
    "$$Y = \\sigma(H \\cdot W_o + b_o) = \\frac{1}{1+e^{-(H \\cdot W_o + b_o)}} = \\begin{bmatrix} \n",
    "y_{11} & y_{12}\\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "y_{n1} & y_{n2} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "With $\\sigma$ the logistic function, and with $Y$ resulting in a $n \\times 2$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def logistic(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to compute the hidden activations\n",
    "def hidden_activations(X, Wh, bh):\n",
    "    return logistic(X * Wh + bh)\n",
    "\n",
    "# Define output layer feedforward\n",
    "def output_activations(H, Wo, bo):\n",
    "    return logistic(H * Wo + bo)\n",
    "\n",
    "# Define the neural network function\n",
    "def nn(X, Wh, bh, Wo, bo): \n",
    "    return output_activations(hidden_activations(X, Wh, bh), Wo, bo)\n",
    "\n",
    "# Define the neural network prediction function that only returns\n",
    "#  1 or 0 depending on the predicted class\n",
    "def nn_predict(X, Wh, bh, Wo, bo): \n",
    "    return np.around(nn(X, Wh, bh, Wo, bo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Weights and biases\n",
    "bh = np.asmatrix([[0, 0, 0]])\n",
    "Wh = np.asmatrix([[1, 1, 1],[1, 1, 1]])\n",
    "\n",
    "bo = np.asmatrix(([0, 0]))\n",
    "Wo = np.asmatrix([[1, 1], [1, 1], [1, 1]])\n",
    "\n",
    "\n",
    "print 'X.shape: ', X.shape\n",
    "print 'Wh.shape: ', Wh.shape\n",
    "print 'bh.shape: ', bh.shape\n",
    "\n",
    "H = hidden_activations(X, Wh, bh)\n",
    "print 'H.shape: ', H.shape\n",
    "print 'Wo.shape: ', Wo.shape\n",
    "print 'bo.shape: ', bo.shape\n",
    "\n",
    "O = output_activations(H, Wo, bo)\n",
    "print 'O.shape: ', O.shape\n",
    "\n",
    "Y = nn(X, Wh, bh, Wo, bo)\n",
    "print 'Y.shape: ', Y.shape\n",
    "\n",
    "Y_pred = nn_predict(X, Wh, bh, Wo, bo)\n",
    "print 'Y_pred.shape: ', Y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vectorization of the backward step\n",
    "\n",
    "#### Vectorization of the output layer backward step\n",
    "\n",
    "##### Compute the error at the output\n",
    "\n",
    "The cost function $\\xi$ for $n$ samples and $c$ classes used in this model is the cross-entropy cost function:\n",
    "\n",
    "$$\\xi(T,Y) = \\sum_{i=1}^n \\xi(\\mathbf{t}_i,\\mathbf{y}_i) = - \\sum_{i=1}^n \\sum_{i=c}^{C} t_{ic} \\cdot log( y_{ic}) $$\n",
    "\n",
    "\n",
    "The error gradient $\\delta_{o}$ of this cost function at the softmax output layer is simply:\n",
    "\n",
    "$$\\delta_{o} = \\frac{\\partial \\xi}{\\partial Z_o} = Y - T$$\n",
    "\n",
    "With $Z_o$ a $n \\times 2$ matrix of inputs to the softmax layer ($Z_o = H \\cdot W_o + b_o$), and $T$ a $n \\times 2$ target matrix that corresponds to $Y$. Note that $\\delta_{o}$ also results in a $n \\times 2$ matrix.\n",
    "\n",
    "##### Update the output layer weights\n",
    "At the output the gradient $\\delta_{w_{oij}}$ is computed by ${\\partial \\xi}/{\\partial w_{oij}}$. We worked out this formula for batch processing over all $n$ samples in part 2:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{oij}} = \n",
    "\\frac{\\partial Z_{o}}{\\partial w_{oij}} \\frac{\\partial Y}{\\partial Z_{o}} \\frac{\\partial \\xi}{\\partial Y} = \n",
    "\\frac{\\partial Z_{o}}{\\partial w_{oij}} \\frac{\\partial \\xi}{\\partial Z_o} =\n",
    "\\sum_{j=1}^n h_{ji} (y_j - t_j) = \\sum_{j=1}^n h_{ji} \\delta_{oj}$$\n",
    "\n",
    "We can write this formula out as matrix operations with the parameters of the output layer:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial W_o} = H^T \\cdot (Y-T) = H^T \\cdot \\delta_{o}$$\n",
    "\n",
    "The resulting gradient is a $3 \\times 2$ [Jacobian matrix](http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant):\n",
    "\n",
    "$$J_{W_o} = \\nabla_{W_o} \\xi =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\xi}{\\partial w_{o11}} & \\frac{\\partial \\xi}{\\partial w_{o12}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{o21}} & \\frac{\\partial \\xi}{\\partial w_{o22}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{o31}} & \\frac{\\partial \\xi}{\\partial w_{o32}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The output weights $W_o$ can then be updated with learning rate $\\mu$ according to:\n",
    "\n",
    "$$ W_o(k+1) = W_o(k) - \\mu * J_{W_o} $$\n",
    "\n",
    "##### Update the output layer bias\n",
    "The bias $b_o$ can be updated in the same manner. The gradient $\\delta_{b_{oi}}$ is computed by ${\\partial \\xi}/{\\partial b_{oi}}$. We worked out this formula for batch processing over all $n$ samples in part 2:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial b_{oi}} = \\frac{\\partial Z_{o}}{\\partial b_{oi}} \\frac{\\partial Y}{\\partial Z_{o}} \\frac{\\partial \\xi}{\\partial Y} = \\sum_{j=1}^n 1 * (y_j - t_j) = \\sum_{j=1}^n \\delta_{oj}$$\n",
    "\n",
    "The resulting gradient is a $2 \\times 1$ Jacobian matrix:\n",
    "\n",
    "$$J_{b_o} = \\nabla_{b_o} \\xi =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\xi}{\\partial b_{o1}} & \\frac{\\partial \\xi}{\\partial b_{o2}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The output bias $b_o$ can then be updated with learning rate $\\mu$ according to:\n",
    "\n",
    "$$ b_o(k+1) = b_o(k) - \\mu * J_{b_o} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "def cost(Y, T):\n",
    "    return - np.sum(np.multiply(T, np.log(Y)) + np.multiply((1-T), np.log(1-Y)))\n",
    "\n",
    "# Define the error function at the output\n",
    "def error_output(Y, T):\n",
    "    return Y - T\n",
    "\n",
    "# Define the gradient function for the weight parameters at the output layer\n",
    "def gradient_weight_out(H, Eo): \n",
    "    return  H.T * Eo\n",
    "\n",
    "# Define the gradient function for the bias parameters at the output layer\n",
    "def gradient_bias_out(Eo): \n",
    "    return  np.sum(Eo, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = cost(Y, T)\n",
    "print 'C.shape: ', C.shape\n",
    "\n",
    "Eo = error_output(Y, T)\n",
    "print 'Eo.shape: ', Eo.shape\n",
    "\n",
    "JWo = gradient_weight_out(H, Eo)\n",
    "print 'JWo.shape: ', JWo.shape\n",
    "\n",
    "Jbo = gradient_bias_out(Eo)\n",
    "print 'Jbo.shape: ', Jbo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization of the hidden layer backward step\n",
    "\n",
    "##### Compute the error at the hidden layer\n",
    "\n",
    "The error gradient $\\delta_{h}$ of the cost function at the hidden layer is defined as:\n",
    "\n",
    "$$\\delta_{h} = \\frac{\\partial \\xi}{\\partial Z_h} = \\frac{\\partial H}{\\partial Z_h} \\frac{\\partial \\xi}{\\partial H} = \\frac{\\partial H}{\\partial Z_h} \\frac{\\partial Z_o}{\\partial H} \\frac{\\partial \\xi}{\\partial Z_o}$$\n",
    "\n",
    "With $Z_h$ a $n \\times 3$ matrix of inputs to the logistic functions in the hidden neurons ($Z_h = X \\cdot W_h + b_h$). Note that $\\delta_{h}$ will also result in a $n \\times 3$ matrix.\n",
    "\n",
    "Lets first derive the error gradient $\\delta_{hij}$ for one sample $i$ at hidden neuron $j$. The gradients that backpropagate from the previous layer via the weighted connections are summed for each origin $h_{ij}$. \n",
    "\n",
    "$$\\delta_{hij} = \\frac{\\partial \\xi}{\\partial z_{hij}} \n",
    "= \\frac{\\partial h_{ij}}{\\partial z_{hij}} \\frac{\\partial z_{oi}}{\\partial h_{ij}} \\frac{\\partial \\xi}{\\partial z_{oi}} \n",
    "= h_{ij} (1-h_{ij}) \\sum_{k=1}^2 w_{ojk} (y_{ik}-t_{ik})\n",
    "= h_{ij} (1-h_{ij}) [\\delta_{oi} \\cdot w_{oj}^T]$$\n",
    "\n",
    "Where $w_{oj}$ is the $j$-th row of $W_o$, and thus a $1 \\times 2$ vector and $\\delta_{oi}$ is a $1 \\times 2$ vector. The full $n \\times 3$ error matrix $\\delta_{h}$ can thus be calculated as:\n",
    "\n",
    "$$\\delta_{h} = \\frac{\\partial \\xi}{\\partial Z_h} = H \\circ (1 - H) \\circ [\\delta_{o} \\cdot W_o^T]$$\n",
    "\n",
    "With $\\circ$ the [elementwise product](http://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29). \n",
    "\n",
    "\n",
    "##### Update the hidden layer weights\n",
    "At the hidden layer the gradient ${\\partial \\xi}/{\\partial w_{hij}}$ of each $w_{hij}$ over all $n$ samples can be computed by:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{hij}} = \n",
    "\\frac{\\partial Z_{h}}{\\partial w_{hij}} \\frac{\\partial H}{\\partial Z_{h}} \\frac{\\partial \\xi}{\\partial H} = \n",
    "\\frac{\\partial Z_{h}}{\\partial w_{hij}} \\frac{\\partial \\xi}{\\partial Z_h} =\n",
    "\\sum_{j=1}^n x_{ji} \\delta_{hj}$$\n",
    "\n",
    "We can write this formula out as matrix operations with the parameters of the hidden layer:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial W_h} = X^T \\cdot \\delta_{h}$$\n",
    "\n",
    "The resulting gradient is a $2 \\times 3$ [Jacobian matrix](http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant):\n",
    "\n",
    "$$J_{W_h} = \\nabla_{W_h} \\xi =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\xi}{\\partial w_{h11}} & \\frac{\\partial \\xi}{\\partial w_{h12}} & \\frac{\\partial \\xi}{\\partial w_{h13}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{h21}} & \\frac{\\partial \\xi}{\\partial w_{h22}} & \\frac{\\partial \\xi}{\\partial w_{h23}} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The output weights $W_h$ can then be updated with learning rate $\\mu$ according to:\n",
    "\n",
    "$$ W_h(k+1) = W_h(k) - \\mu * J_{W_h} $$\n",
    "\n",
    "##### Update the hidden layer bias\n",
    "The bias $b_h$ can be updated in the same manner. The gradient $\\delta_{b_{hi}}$ is computed by ${\\partial \\xi}/{\\partial b_{hi}}$. We worked out this formula for batch processing over all $n$ samples in part 2:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial b_{hi}} = \\frac{\\partial Z_{h}}{\\partial b_{hi}} \\frac{\\partial H}{\\partial Z_{h}} \\frac{\\partial \\xi}{\\partial H} \n",
    "= \\sum_{j=1}^n \\delta_{hj}$$\n",
    "\n",
    "The resulting gradient is a $1 \\times 3$ Jacobian matrix:\n",
    "\n",
    "$$J_{b_h} = \\nabla_{b_h} \\xi =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\xi}{\\partial b_{h1}} & \\frac{\\partial \\xi}{\\partial b_{h2}} & \\frac{\\partial \\xi}{\\partial b_{h3}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The output bias $b_h$ can then be updated with learning rate $\\mu$ according to:\n",
    "\n",
    "$$ b_h(k+1) = b_h(k) - \\mu * J_{b_h} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the error function at the hidden layer\n",
    "def error_hidden(H, Wo, Eo):\n",
    "    # H * (1-H) * (E . Wo^T)\n",
    "    return np.multiply(np.multiply(H,(1 - H)), Eo.dot(Wo.T))\n",
    "\n",
    "# Define the gradient function for the weight parameters at the hidden layer\n",
    "def gradient_weight_hidden(X, Eh):\n",
    "    return X.T * Eh\n",
    "\n",
    "# Define the gradient function for the bias parameters at the output layer\n",
    "def gradient_bias_hidden(Eh): \n",
    "    return  np.sum(Eh, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Eh = error_hidden(H, Wo, Eo)\n",
    "print 'Eh.shape: ', Eh.shape\n",
    "\n",
    "JWh = gradient_weight_hidden(X, Eh)\n",
    "print 'JWh.shape: ', JWh.shape\n",
    "\n",
    "Jbh = gradient_bias_hidden(Eh)\n",
    "print 'Jbh.shape: ', Jbh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checking\n",
    "\n",
    "Programming the computation of the backpropagation gradient is prone to bugs. This is why it is recommended to [check the gradients](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization) of your models. Gradient checking is done by computing the [numerical gradient](http://en.wikipedia.org/wiki/Numerical_differentiation) of each parameter, and compare this value to the gradient found by backpropagation.\n",
    "\n",
    "The numerical gradient ${\\partial \\xi}/{\\partial \\theta_i}$ for a parameter $\\theta_i$ can be computed by:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial \\theta_i}\n",
    "= \\frac{f(\\theta_1, \\cdots, \\theta_i+\\epsilon, \\cdots, \\theta_m) - f(\\theta_1, \\cdots, \\theta_i-\\epsilon, \\cdots, \\theta_m)}{2\\epsilon}$$\n",
    "\n",
    "Where $f$ if the neural network output function that takes all parameters $\\theta$, and $\\epsilon$ is the small change that is used to peturbate the parameter $\\theta_i$. \n",
    "\n",
    "The numerical gradient for each parameter should be close to the backpropagation gradient for that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize weights and biases\n",
    "init_var = 0.1\n",
    "# Initialize hidden layer parameters\n",
    "bh = np.random.randn(1, 3) * init_var\n",
    "Wh = np.random.randn(2, 3) * init_var\n",
    "# Initialize output layer parameters\n",
    "bo = np.random.randn(1, 2) * init_var\n",
    "Wo = np.random.randn(3, 2) * init_var\n",
    "\n",
    "# Compute the gradients by backpropagation\n",
    "# Compute the activations of the layers\n",
    "H = hidden_activations(X, Wh, bh)\n",
    "Y = output_activations(H, Wo, bo)\n",
    "# Compute the gradients of the output layer\n",
    "Eo = error_output(Y, T)\n",
    "JWo = gradient_weight_out(H, Eo)\n",
    "Jbo = gradient_bias_out(Eo)\n",
    "# Compute the gradients of the hidden layer\n",
    "Eh = error_hidden(H, Wo, Eo)\n",
    "JWh = gradient_weight_hidden(X, Eh)\n",
    "Jbh = gradient_bias_hidden(Eh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine all parameter matrices in a list\n",
    "params = [Wh, bh, Wo, bo]\n",
    "# Combine all parameter gradients in a list\n",
    "grad_params = [JWh, Jbh, JWo, Jbo]\n",
    "\n",
    "# Set the small change to compute the numerical gradient\n",
    "eps = 0.0001\n",
    "\n",
    "# Check each parameter matrix\n",
    "for p_idx in xrange(len(params)):\n",
    "    # Check each parameter in each parameter matrix\n",
    "    for row in xrange(params[p_idx].shape[0]):\n",
    "        for col in xrange(params[p_idx].shape[1]):\n",
    "            # Copy the parameter matrix and change the current parameter slightly\n",
    "            p_matrix_min = params[p_idx].copy()\n",
    "            p_matrix_min[row,col] -= eps\n",
    "            p_matrix_plus = params[p_idx].copy()\n",
    "            p_matrix_plus[row,col] += eps\n",
    "            # Copy the parameter list, and change the updated parameter matrix\n",
    "            params_min = params[:]\n",
    "            params_min[p_idx] = p_matrix_min\n",
    "            params_plus = params[:]\n",
    "            params_plus[p_idx] =  p_matrix_plus\n",
    "            # Compute the numerical gradient\n",
    "            grad_num = (cost(nn(X, *params_plus), T)-cost(nn(X, *params_min), T))/(2*eps)\n",
    "            print 'backprop gradient: {:.6f} ; numerical gradient {:.6f}'.format(grad_params[p_idx][row,col], grad_num)\n",
    "            if not np.isclose(grad_num, grad_params[p_idx][row,col]):\n",
    "                raise ValueError('Numerical gradient is not close to the backpropagation gradient!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the update function to update the network parameters over 1 iteration\n",
    "def backprop_gradients(X, T, Wh, bh, Wo, bo):\n",
    "    # Compute the output of the network\n",
    "    # Compute the activations of the layers\n",
    "    H = hidden_activations(X, Wh, bh)\n",
    "    Y = output_activations(H, Wo, bo)\n",
    "    # Compute the gradients of the output layer\n",
    "    Eo = error_output(Y, T)\n",
    "    JWo = gradient_weight_out(H, Eo)\n",
    "    Jbo = gradient_bias_out(Eo)\n",
    "    # Compute the gradients of the hidden layer\n",
    "    Eh = error_hidden(H, Wo, Eo)\n",
    "    JWh = gradient_weight_hidden(X, Eh)\n",
    "    Jbh = gradient_bias_hidden(Eh)\n",
    "    return [JWh, Jbh, JWo, Jbo]\n",
    "\n",
    "def update_velocity(X, T, ls_of_params, Vs, momentum_term, learning_rate):\n",
    "    # ls_of_params = [Wh, bh, Wo, bo]\n",
    "    # Js = [JWh, Jbh, JWo, Jbo]\n",
    "    Js = backprop_gradients(X, T, *ls_of_params)\n",
    "    return [momentum_term * V + learning_rate * J for V,J in zip(Vs, Js)]\n",
    "\n",
    "def update_params(ls_of_params, Vs):\n",
    "    # ls_of_params = [Wh, bh, Wo, bo]\n",
    "    # Vs = [VWh, Vbh, VWo, Vbo]\n",
    "    return [P - V for P,V in zip(ls_of_params, Vs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run backpropagation\n",
    "# Initialize weights and biases\n",
    "init_var = 0.1\n",
    "# Initialize hidden layer parameters\n",
    "bh = np.random.randn(1, 3) * init_var\n",
    "Wh = np.random.randn(2, 3) * init_var\n",
    "# Initialize output layer parameters\n",
    "bo = np.random.randn(1, 2) * init_var\n",
    "Wo = np.random.randn(3, 2) * init_var\n",
    "# Parameters are already initilized randomly with the gradient checking\n",
    "# Set the learning rate\n",
    "learning_rate = 0.05\n",
    "momentum_term = 0.9\n",
    "\n",
    "# define the velocities Vs = [VWh, Vbh, VWo, Vbo]\n",
    "Vs = [np.zeros_like(M) for M in [Wh, bh, Wo, bo]]\n",
    "\n",
    "# Start the gradient descent updates and plot the iterations\n",
    "nb_of_iterations = 200  # number of gradient descent updates\n",
    "lr_update = learning_rate / nb_of_iterations # learning rate update rule\n",
    "ls_costs = []  # list of cost over the iterations\n",
    "\n",
    "for i in xrange(nb_of_iterations):\n",
    "    # Add the current cost to the cost list\n",
    "    current_cost = cost(nn(X, Wh, bh, Wo, bo), T)\n",
    "    ls_costs.append(current_cost)\n",
    "    Vs = update_velocity(X, T, [Wh, bh, Wo, bo], Vs, momentum_term, learning_rate)s\n",
    "    Wh, bh, Wo, bo = update_params([Wh, bh, Wo, bo], Vs)\n",
    "\n",
    "# Add the final cost to the cost list\n",
    "ls_costs.append(cost(nn(X, Wh, bh, Wo, bo), T))\n",
    "# Plot the cost over the iterations\n",
    "plt.plot(ls_costs, 'b-')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('Decrease of cost over backprop iteration')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the resulting decision boundary\n",
    "# Generate a grid over the input space to plot the color of the\n",
    "#  classification at that grid point\n",
    "nb_of_xs = 200\n",
    "xs1 = np.linspace(-2, 2, num=nb_of_xs)\n",
    "xs2 = np.linspace(-2, 2, num=nb_of_xs)\n",
    "xx, yy = np.meshgrid(xs1, xs2) # create the grid\n",
    "# Initialize and fill the classification plane\n",
    "classification_plane = np.zeros((nb_of_xs, nb_of_xs))\n",
    "for i in xrange(nb_of_xs):\n",
    "    for j in xrange(nb_of_xs):\n",
    "        pred = nn_predict(np.asmatrix([xx[i,j], yy[i,j]]), Wh, bh, Wo, bo)\n",
    "        classification_plane[i,j] = pred[0,0]\n",
    "# Create a color map to show the classification colors of each grid point\n",
    "cmap = ListedColormap([\n",
    "        colorConverter.to_rgba('r', alpha=0.30),\n",
    "        colorConverter.to_rgba('b', alpha=0.30)])\n",
    "\n",
    "# Plot the classification plane with decision boundary and input samples\n",
    "plt.contourf(xx, yy, classification_plane, cmap=cmap)\n",
    "# Plot both classes on the x1, x2 plane\n",
    "plt.plot(x_red[:,0], x_red[:,1], 'ro', label='class red')\n",
    "plt.plot(x_blue[:,0], x_blue[:,1], 'bo', label='class blue')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('$x_1$', fontsize=15)\n",
    "plt.ylabel('$x_2$', fontsize=15)\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "plt.title('red vs blue classification boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H_blue = hidden_activations(x_blue, Wh, bh)\n",
    "H_red = hidden_activations(x_red, Wh, bh)\n",
    "\n",
    "# Plot the error surface\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.plot(np.ravel(H_blue[:,0]), np.ravel(H_blue[:,1]), np.ravel(H_blue[:,2]), 'bo')\n",
    "ax.plot(np.ravel(H_red[:,0]), np.ravel(H_red[:,1]), np.ravel(H_red[:,2]), 'ro')\n",
    "ax.set_xlabel('$h_1$', fontsize=15)\n",
    "ax.set_ylabel('$h_2$', fontsize=15)\n",
    "ax.set_zlabel('$h_3$', fontsize=15)\n",
    "ax.view_init(elev=10, azim=-30)\n",
    "plt.title('Projection of the input X onto the hidden layer H')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
