{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Neural network\n",
    "## Part 4: Vectorization\n",
    "\n",
    "This tutorial is part 3 of the previous tutorials on neural networks (TODO: url). While the previous tutorials described very simple single layer regression and classification models, this tutorial will describe a 2-class classification neural network with 1 input dimension, and a non-linear hidden layer with 2 dimensions. While we didn't add the bias parameters to the previous 2 models, we will add them to this model. The network of this model is shown in the following figure:\n",
    "\n",
    "![Image of the logistic model](https://dl.dropboxusercontent.com/u/8938051/Blog_images/SimpleANN03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset \n",
    "\n",
    "In this example the target classes $t$ will be generated from 2 class distributions: blue ($t=1$) and red ($t=0$). Where the red class is a circular distribution that surrounds the distribution of the blue class. This results in a 2D dataset that is not linearly seperable. The model from part 2 won't be able to classify both classes correctly since it can learn only linear seperators. By adding a hidden layer the model will be able to train a non-linear seperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and generate the samples\n",
    "nb_of_samples_per_class = 50  # The number of sample in each class\n",
    "\n",
    "# Generate blue samples\n",
    "blue_mean = [0,0]  # The mean of the blue class\n",
    "blue_std_dev = 0.3  # standard deviation of blue class\n",
    "x_blue = np.random.randn(nb_of_samples_per_class, 2) * blue_std_dev + blue_mean\n",
    "\n",
    "# Generate red samples as circle around blue samples\n",
    "red_radius_mean = 1.3  # mean of the radius\n",
    "red_radius_std_dev = 0.2  # standard deviation of the radius\n",
    "red_rand_radius = np.random.randn(nb_of_samples_per_class) * red_radius_std_dev + red_radius_mean\n",
    "red_rand_angle = 2 * np.pi * np.random.rand(nb_of_samples_per_class);\n",
    "x_red = np.asmatrix([red_rand_radius * np.cos(red_rand_angle), \n",
    "                     red_rand_radius * np.sin(red_rand_angle)]).T\n",
    "\n",
    "# Define target vectors for blue and red\n",
    "t_blue_vector = np.asarray([1, 0])\n",
    "t_red_vector = np.asarray([0, 1])\n",
    "# Define the full target matrix for each class\n",
    "t_blue = np.tile(t_blue_vector, (nb_of_samples_per_class, 1))\n",
    "t_red = np.tile(t_red_vector, (nb_of_samples_per_class, 1))\n",
    "\n",
    "# Merge samples in set of input variables x, and corresponding set of\n",
    "# output variables t\n",
    "X = np.vstack((x_blue, x_red))\n",
    "T = np.vstack((t_blue, t_red))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot both classes on the x1, x2 plane\n",
    "plt.plot(x_red[:,0], x_red[:,1], 'ro', label='class red')\n",
    "plt.plot(x_blue[:,0], x_blue[:,1], 'bo', label='class blue')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "# plt.axis([-4, 4, -4, 4])\n",
    "plt.title('red vs blue classes in the input space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of backpropagation\n",
    "\n",
    "### 1. Vectorization of the forward step\n",
    "\n",
    "#### Compute activations of hidden layer\n",
    "The $n$ input samples with $2$ variables each are given as a $n \\times 2$ matrix $X$:\n",
    "\n",
    "$$X =\n",
    "\\begin{bmatrix} \n",
    "x_{11} & x_{12} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_{n1} & x_{n2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Where $x_{ij}$ is the value of the $j$-th variable of the $i$-th input sample. These inputs are projected onto the 3 dimensions of the hidden layer $H$ by the weight matrix $W$ ($w_{hij}$ is the weight of the connection between input variable $i$ and hidden neuron activation $j$) and bias vector $b$ :\n",
    "\n",
    "$$\\begin{align}\n",
    "W_h =\n",
    "\\begin{bmatrix} \n",
    "w_{h11} & w_{h12} & w_{h13} \\\\\n",
    "w_{h21} & w_{h22} & w_{h23}\n",
    "\\end{bmatrix}\n",
    "&& b_h = \n",
    "\\begin{bmatrix} \n",
    "b_{h1} \\\\\n",
    "b_{h2} \\\\\n",
    "b_{h3}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "following computation: \n",
    "\n",
    "$$H = \\sigma(X \\cdot W_h + b_h) = \\frac{1}{1+e^{-(X \\cdot W_h + b_h)}} = \\begin{bmatrix} \n",
    "h_{11} & h_{12} & h_{13} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "h_{n1} & h_{n2} & h_{n3}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "With $\\sigma$ the logistic function, and with $H$ resulting in a $n \\times 3$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute activations of output \n",
    "\n",
    "To compute the output activations the hidden layer activations can be projected onto the 2 dimensional output layer. This is done by the $3 \\times 2$ weight matrix $W_o$ ($w_{oij}$ is the weight of the connection between hidden layer neuron $i$ and output activation $j$) and $2 \\times 1$ bias vector $b_o$ :\n",
    "\n",
    "$$\\begin{align}\n",
    "W_o =\n",
    "\\begin{bmatrix} \n",
    "w_{o11} & w_{o12} \\\\\n",
    "w_{o21} & w_{o22} \\\\\n",
    "w_{o31} & w_{o32}\n",
    "\\end{bmatrix}\n",
    "&& b_o = \n",
    "\\begin{bmatrix} \n",
    "b_{o1} \\\\\n",
    "b_{o2}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "following computation: \n",
    "\n",
    "$$Y = \\sigma(H \\cdot W_o + b_o) = \\frac{1}{1+e^{-(H \\cdot W_o + b_o)}} = \\begin{bmatrix} \n",
    "y_{11} & y_{12}\\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "y_{n1} & y_{n2} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "With $\\sigma$ the logistic function, and with $Y$ resulting in a $n \\times 2$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def logistic(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to compute the hidden activations\n",
    "def hidden_activations(X, Wh, bh):\n",
    "    return logistic(X * Wh + bh)\n",
    "\n",
    "# Define output layer feedforward\n",
    "def output_activations(H, Wo, bo):\n",
    "    return logistic(H * Wo + bo)\n",
    "\n",
    "# Define the neural network function\n",
    "def nn(X, Wh, bh, Wo, bo): \n",
    "    return output_activations(hidden_activations(X, Wh, bh), Wo, bo)\n",
    "\n",
    "# Define the neural network prediction function that only returns\n",
    "#  1 or 0 depending on the predicted class\n",
    "def nn_predict(X, Wh, bh, Wo, bo): \n",
    "    return np.around(nn(X, Wh, bh, Wo, bo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Weights and biases\n",
    "bh = np.asarray([0, 0, 0])\n",
    "Wh = np.asmatrix([[1, 1, 1],[1, 1, 1]])\n",
    "\n",
    "bo = np.asarray([0, 0])\n",
    "Wo = np.asmatrix([[1, 1], [1, 1], [1, 1]])\n",
    "\n",
    "\n",
    "print 'X.shape: ', X.shape\n",
    "print 'Wh.shape: ', Wh.shape\n",
    "print 'bh.shape: ', bh.shape\n",
    "\n",
    "H = hidden_activations(X, Wh, bh)\n",
    "print 'H.shape: ', H.shape\n",
    "print 'Wo.shape: ', Wo.shape\n",
    "print 'bo.shape: ', bo.shape\n",
    "\n",
    "O = output_activations(H, Wo, bo)\n",
    "print 'O.shape: ', O.shape\n",
    "\n",
    "Y = nn(X, Wh, bh, Wo, bo)\n",
    "print 'Y.shape: ', Y.shape\n",
    "\n",
    "Y_pred = nn_predict(X, Wh, bh, Wo, bo)\n",
    "print 'Y_pred.shape: ', Y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vectorization of the backward step\n",
    "\n",
    "#### Compute the error at the output\n",
    "\n",
    "The cost function $\\xi$ for $n$ samples and $c$ classes used in this model is the cross-entropy cost function:\n",
    "\n",
    "$\\xi(T,Y) = \\sum_{i=1}^n \\xi(\\mathbf{t}_i,\\mathbf{y}_i) = - \\sum_{i=1}^n \\sum_{i=c}^{C} t_{ic} \\cdot log( y_{ic}) $\n",
    "\n",
    "\n",
    "\n",
    "The error gradient $\\delta_{o}$ of this cost function at the softmax output layer is simply:\n",
    "\n",
    "$$\\delta_{o} = \\frac{\\partial \\xi}{\\partial Z_o} = Y - T$$\n",
    "\n",
    "With $Z_o$ a $n \\times 2$ matrix of inputs to the softmax layer ($Z_o = H \\cdot W_o + b_o$), anc $T$ a $n \\times 2$ target matrix that corresponds to $Y$. Note that $\\delta_{o}$ is a \n",
    "\n",
    "#### Update the output layer weights\n",
    "At the output the gradient $\\delta_{w_{oij}}$ is computed by ${\\partial \\xi}/{\\partial w_{oij}}$. We worked out this formula for batch processing over all $n$ samples in part 2:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{oij}} = \\frac{\\partial Z_{o}}{\\partial w_{oij}} \\frac{\\partial Y}{\\partial Z_{o}} \\frac{\\partial \\xi}{\\partial Y} = \\sum_{j=1}^n h_{ji} (y_j - t_j) = \\sum_{j=1}^n h_{ji} \\delta_{oj}$$\n",
    "\n",
    "We can write this formula out as matrix operations with the parameters of the output layer:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial W_o} = H^T \\cdot (Y-T) = H^T \\cdot \\delta_{o}$$\n",
    "\n",
    "The resulting gradient is a $3 \\times 2$ [Jacobian matrix](http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant):\n",
    "\n",
    "$$J_{W_o} = \\nabla_{W_o} \\xi =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\xi}{\\partial w_{o11}} & \\frac{\\partial \\xi}{\\partial w_{o12}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{o21}} & \\frac{\\partial \\xi}{\\partial w_{o22}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{o31}} & \\frac{\\partial \\xi}{\\partial w_{o32}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The output weights $W_o$ can then be updated with learning rate $\\mu$ according to:\n",
    "\n",
    "$$ W_o(k+1) = W_o(k) - \\mu * J_{W_o} $$\n",
    "\n",
    "#### Update the output layer bias\n",
    "The bias $b_o$ can be updated in the same manner. The gradient $\\delta_{b_{oi}}$ is computed by ${\\partial \\xi}/{\\partial b_{oi}}$. We worked out this formula for batch processing over all $n$ samples in part 2:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial b_{oi}} = \\frac{\\partial Z_{o}}{\\partial b_{oi}} \\frac{\\partial Y}{\\partial Z_{o}} \\frac{\\partial \\xi}{\\partial Y} = \\sum_{j=1}^n 1 * (y_j - t_j) = \\sum_{j=1}^n \\delta_{oj}$$\n",
    "\n",
    "The resulting gradient is a $2 \\times 1$ Jacobian matrix:\n",
    "\n",
    "$$J_{b_o} = \\nabla_{b_o} \\xi =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\xi}{\\partial b_{o1}}\\\\\n",
    "\\frac{\\partial \\xi}{\\partial b_{o2}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The output bias $b_o$ can then be updated with learning rate $\\mu$ according to:\n",
    "\n",
    "$$ b_o(k+1) = b_o(k) - \\mu * J_{b_o} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the error at the hidden layer\n",
    "\n",
    "The error gradient $\\delta_{h}$ of the cost function at the hidden layer is defined as:\n",
    "\n",
    "$$\\delta_{h} = \\frac{\\partial \\xi}{\\partial Z_h} = \\frac{\\partial H}{\\partial Z_h} \\frac{\\partial \\xi}{\\partial H} = \\frac{\\partial H}{\\partial Z_h} \\frac{\\partial Z_o}{\\partial H}\\frac{\\partial \\xi}{\\partial Z_o}$$\n",
    "\n",
    "With $Z_o$ a $n \\times 2$ matrix of inputs to the softmax layer ($Z_o = H \\cdot W_o + b_o$), anc $T$ a $n \\times 2$ target matrix that corresponds to $Y$. Note that $\\delta_{o}$ is a \n",
    "\n",
    "\n",
    "\n",
    "$$\\delta_{h} = \\frac{\\partial \\xi}{\\partial h} = \\frac{\\partial z_{o}}{\\partial h} \\frac{\\partial y}{\\partial z_{o}} \\frac{\\partial \\xi}{\\partial y} = w_{o} (y - t) = w_{o} \\delta_{o} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Backward step\n",
    "\n",
    "The backward step will begin with computing the cost at the output node. This cost will then be propagated backwards layer by layer through the network to update the parameters.\n",
    "\n",
    "The [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) algorithm is used in every layer to update the parameters in the direction of the negative [gradient](http://en.wikipedia.org/wiki/Gradient).\n",
    "\n",
    "The parameters $w$ are updated by $w(k+1) = w(k) - \\Delta w(k+1)$. $\\Delta w$ is defined as: $\\Delta w = \\mu \\delta_{w}$ with $\\mu$ the learning rate and $\\delta_{w}$ the local gradient at the output of a neuron that has $w$ as a parameter.\n",
    "\n",
    "#### Compute the cost function\n",
    "\n",
    "The cost function $\\xi$ used in this model is the same cross-entropy cost function used in part 2:\n",
    "\n",
    "$$\\xi(T,Y) = - \\sum_{i=1}^{n} \\left[ t_i log(y_i) + (1-t_i)log(1-y_i) \\right]$$\n",
    "\n",
    "\n",
    "#### Update the output layer\n",
    "\n",
    "At the output the gradient $\\delta_{w}$ is computed by ${\\partial \\xi}/{\\partial w}$. We worked out this formula for batch processing in part 2:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{oi}} = \\frac{\\partial z_{o}}{\\partial w_{oi}} \\frac{\\partial y}{\\partial z_{o}} \\frac{\\partial \\xi}{\\partial y} = \\sum_{j=1}^n h_{ji} (y_j - t_j)$$\n",
    "\n",
    "With $z_{o} = H * W_o$. We can write this formula out as matrix operations with the parameters of the output layer:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial W_o} = H^T \\cdot (Y-T) $$\n",
    "\n",
    "The resulting gradient is a $3 \\times 1$ [Jacobian matrix](http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant):\n",
    "\n",
    "$$J_o =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\xi}{\\partial b_o} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{o1}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{o2}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "#### Update the input layer\n",
    "\n",
    "At the hidden layer the gradient $\\delta_{w}$ of the neuron with parameter $w$ is computed the same way:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{hij}} = \\frac{\\partial z_{hj}}{\\partial w_{hij}} \\frac{\\partial h_j}{\\partial z_{hj}} \\frac{\\partial \\xi}{\\partial h_j}$$\n",
    "\n",
    "With for all samples $k$: $z_{hj} = \\sum_{i=0}^3 x_{ki} * w_{hij} $. And with ${\\partial \\xi}/{\\partial h_j}$ the gradient of the error at the output of the hidden node $h_j$ with respect to this $h_j$. This error can be interpreted as the contribution of $h_j$ to the total error.\n",
    "How do we define this error at the output of the hidden nodes? For each output neuron $h_j$ we can compute this as the sum of the errors propagated back from the connections going out of $h_j$:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial h_j} = \\frac{\\partial z_{o}}{\\partial h_j} \\frac{\\partial \\xi}{\\partial z_{o}} = w_{oj} \\frac{\\partial \\xi}{\\partial z_{o}}$$\n",
    "\n",
    "Because of this, and because ${\\partial z_{hj}}/{\\partial w_{hij}} = x_{i}$ and ${\\partial h_j}/{\\partial z_{hj}} = h_j * (1-h_j)$ and ${\\partial \\xi}/{\\partial z_{o}} = y - t = E$ we compute ${\\partial \\xi}/{\\partial w_{hij}}$ over all samples $k$ as:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{hij}} = \\sum_{k=1}^n x_{ki} * h_{kj} * (1-h_{kj}) * w_{oj}  * (y_k-t_k)$$\n",
    "\n",
    "This can be written in matrix format as:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial W_h} = X^T \\cdot [H \\circ (1-H) \\circ (Y - T) * W_o]$$\n",
    "\n",
    "With $\\circ$ the [elementwise product](http://en.wikipedia.org/wiki/Hadamard_product).\n",
    "\n",
    "The resulting gradient is a $3 \\times 2$ [Jacobian matrix](http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant):\n",
    "\n",
    "$$J_o =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\xi}{\\partial b_{h1}} & \\frac{\\partial \\xi}{\\partial b_{h2}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{h11}} & \\frac{\\partial \\xi}{\\partial w_{h12}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{h21}} & \\frac{\\partial \\xi}{\\partial w_{h22}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "To start out the gradient descent algorithm, you typically start with picking the initial parameters at random and start updating these parameters in the direction of the negative gradient with help of the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "def cost(Y, T):\n",
    "    return - np.sum(np.multiply(T, np.log(Y)) + np.multiply((1-T), np.log(1-Y)))\n",
    "\n",
    "# Define the error function\n",
    "def error(Y, T):\n",
    "    return Y - T\n",
    "\n",
    "# define the gradient function for the output layer\n",
    "def gradient_output(H, E): \n",
    "    return  H.T * E\n",
    "\n",
    "def gradient_input(x, wh, bh, error_gradient):\n",
    "    return np.sum(x * error_gradient, axis=0)\n",
    "\n",
    "# # define the update function delta w which returns the \n",
    "# #  delta w for each weight in a vector\n",
    "# def delta_w(w_k, x, t, learning_rate):\n",
    "#     return learning_rate * gradient(w_k, x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = cost(Y, T)\n",
    "print 'C.shape: ', C.shape\n",
    "\n",
    "E = error(Y, T)\n",
    "print 'E.shape: ', E.shape\n",
    "\n",
    "print 'H.shape: ', H.shape\n",
    "Jo = gradient_output(H, E)\n",
    "print 'Jo.shape: ', Jo.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
