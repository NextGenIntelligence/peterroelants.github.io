{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Neural network\n",
    "## Part 3: Hidden layer\n",
    "\n",
    "This tutorial is part 3 of the previous tutorials on neural networks (TODO: url). While the previous tutorials described very simple single layer regression and classification models, this tutorial will describe a 2-class classification neural network with 1 input dimension, and a non-linear hidden layer with 2 dimensions. While we didn't add the bias parameters to the previous 2 models, we will add them to this model. The network of this model is shown in the following figure:\n",
    "\n",
    "![Image of the logistic model](https://dl.dropboxusercontent.com/u/8938051/Blog_images/SimpleANN03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset \n",
    "\n",
    "In this example the target classes $t$ will be generated from 2 class distributions: blue ($t=1$) and red ($t=0$). Where the red class is a circular distribution that surrounds the distribution of the blue class. This results in a 2D dataset that is not linearly seperable. The model from part 2 won't be able to classify both classes correctly since it can learn only linear seperators. By adding a hidden layer the model will be able to train a non-linear seperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and generate the samples\n",
    "nb_of_samples_per_class = 50  # The number of sample in each class\n",
    "\n",
    "# Generate blue samples\n",
    "blue_mean = [0,0]  # The mean of the blue class\n",
    "blue_std_dev = 0.3  # standard deviation of blue class\n",
    "x_blue = np.random.randn(nb_of_samples_per_class, 2) * blue_std_dev + blue_mean\n",
    "\n",
    "# Generate red samples as circle around blue samples\n",
    "red_radius_mean = 1.3  # mean of the radius\n",
    "red_radius_std_dev = 0.2  # standard deviation of the radius\n",
    "red_rand_radius = np.random.randn(nb_of_samples_per_class) * red_radius_std_dev + red_radius_mean\n",
    "red_rand_angle = 2 * np.pi * np.random.rand(nb_of_samples_per_class);\n",
    "x_red = np.asmatrix([red_rand_radius * np.cos(red_rand_angle), \n",
    "                     red_rand_radius * np.sin(red_rand_angle)]).T\n",
    "\n",
    "# Merge samples in set of input variables x, and corresponding set of\n",
    "# output variables t\n",
    "x = np.vstack((x_blue, x_red))\n",
    "T = np.vstack((np.ones((x_blue.shape[0],1)), np.zeros((x_red.shape[0],1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot both classes on the x1, x2 plane\n",
    "plt.plot(x_red[:,0], x_red[:,1], 'ro', label='class red')\n",
    "plt.plot(x_blue[:,0], x_blue[:,1], 'bo', label='class blue')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "# plt.axis([-4, 4, -4, 4])\n",
    "plt.title('red vs blue classes in the input space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization by backpropagation\n",
    "\n",
    "We will train this model by using the [backpropagation](http://en.wikipedia.org/wiki/Backpropagation) algorithm that is typically used to train neural networks. Each step in the backpropagation algorithm consists of two steps:\n",
    "\n",
    "1. A forward propagation step to compute the output of the network.\n",
    "2. A backward propagation step in which the error at the end of the network is propagated backwards through all the neurons, while updating their parameters.\n",
    "\n",
    "### 1. Forward step\n",
    "\n",
    "During the forward step the input will be propagated layer by layer through the network to compute the final output of the network.\n",
    "\n",
    "#### Compute activations of hidden layer\n",
    "The $n$ input samples with $2$ variables each are given as a $n \\times 2$ matrix $X = [[x_{11},x_{12}] \\ldots [x_{n1},x_{n2}]]$ where $x_{ij}$ is the value of the $j$-th variable of the $i$-th input sample. These inputs are projected onto the 2 dimension of the hidden layer $H$ by following computation: \n",
    "\n",
    "$$H = \\sigma(X \\cdot W_h + b_h) = \\frac{1}{1+e^{-(X \\cdot W_h + b_h)}} $$\n",
    "\n",
    "With $H$ resulting in a $n \\times 2$ matrix, and where $W_h = [[w_{h11}, w_{h12}],[w_{h21}, w_{h22}]]$ the $2 \\times 2$ weight matrix ($w_{hij}$ is the weight of the connection between input variable $i$ and hidden neuron activation $j$), and $b_h = [b_{h1}, b_{h2}]^T$ the $1 \\times 2$ bias vector ($b_{hi}$ is the bias of hidden neuron activation $i$).\n",
    "\n",
    "Note that by adding a column of $1$s we can rewrite $X$ as a $n \\times 3$ matrix:\n",
    "\n",
    "$$X =\n",
    "\\begin{bmatrix} \n",
    "1 & x_{11} & x_{12} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{n1} & x_{n2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "And we can incorporate the bias $b_h$ into $W_h$ as a new $3 \\times 2$ matrix:\n",
    "\n",
    "$$W_h =\n",
    "\\begin{bmatrix} \n",
    "b_{h1} & b_{h2} \\\\\n",
    "w_{h11} & w_{h12} \\\\\n",
    "w_{h21} & w_{h22}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "After which we can simplify the computation of $H$ by:\n",
    "\n",
    "$$H = \\frac{1}{1+e^{-(X \\cdot W_h)}} $$\n",
    "\n",
    "\n",
    "#### Compute activations of output \n",
    "\n",
    "To compute the output activations we can use the same trick as above to simplify the equations, we can add a column of  $1$s so can rewrite $H$ as a $n \\times 3$ matrix:\n",
    "\n",
    "$$H =\n",
    "\\begin{bmatrix} \n",
    "1 & h_{11} & h_{12} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & h_{n1} & h_{n2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and write $W_o$ as a $3 \\times 1$ matrix:\n",
    "\n",
    "$$W_o =\n",
    "\\begin{bmatrix} \n",
    "b_o \\\\\n",
    "w_{o1} \\\\\n",
    "w_{o2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The output activations $Y = [y_{1} \\ldots y_{n}]^T$ are then computed from the hidden activations by the output layer according to:\n",
    "\n",
    "$$Y = \\sigma(H \\cdot W_o) = \\frac{1}{1+e^{-(H \\cdot W_o)}} $$\n",
    "\n",
    "With $Y$ resulting in a $n \\times 1$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a column of ones to X\n",
    "X = np.hstack((np.ones((x.shape[0],1)),x))\n",
    "\n",
    "# Define the logistic function\n",
    "def logistic(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to compute the hidden activations\n",
    "def hidden_activations(X, Wh):\n",
    "    H = logistic(X * Wh)\n",
    "    # Add a column of ones to H\n",
    "    return np.hstack((np.ones((H.shape[0],1)),H))\n",
    "\n",
    "# Define output layer feedforward\n",
    "def output_activations(H, Wo):\n",
    "    return logistic(H * Wo)\n",
    "\n",
    "# Define the neural network function\n",
    "def nn(X, Wh, Wo): \n",
    "    return output_activations(hidden_activations(X, Wh), Wo)\n",
    "\n",
    "# Define the neural network prediction function that only returns\n",
    "#  1 or 0 depending on the predicted class\n",
    "def nn_predict(X, Wh, Wo): \n",
    "    return np.around(nn(X, Wh, Wo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Weights and biases\n",
    "bh = np.asarray([0, 0])\n",
    "Wh = np.asmatrix([bh, [1, 1],[1, 1]])\n",
    "\n",
    "bo = np.asarray([0])\n",
    "Wo = np.asmatrix([bo, [1], [1]])\n",
    "\n",
    "\n",
    "print 'X.shape: ', X.shape\n",
    "print 'Wh.shape: ', Wh.shape\n",
    "print 'bh.shape: ', bh.shape\n",
    "\n",
    "H = hidden_activations(X, Wh)\n",
    "print 'H.shape: ', H.shape\n",
    "print 'Wo.shape: ', Wo.shape\n",
    "print 'bo.shape: ', bo.shape\n",
    "\n",
    "O = output_activations(H, Wo)\n",
    "print 'O.shape: ', O.shape\n",
    "\n",
    "Y = nn(X, Wh, Wo)\n",
    "print 'Y.shape: ', Y.shape\n",
    "\n",
    "Y_pred = nn_predict(X, Wh, Wo)\n",
    "print 'Y_pred.shape: ', Y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Backward step\n",
    "\n",
    "The backward step will begin with computing the cost at the output node. This cost will then be propagated backwards layer by layer through the network to update the parameters.\n",
    "\n",
    "The [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) algorithm is used in every layer to update the parameters in the direction of the negative [gradient](http://en.wikipedia.org/wiki/Gradient).\n",
    "\n",
    "The parameters $w$ are updated by $w(k+1) = w(k) - \\Delta w(k+1)$. $\\Delta w$ is defined as: $\\Delta w = \\mu \\delta_{w}$ with $\\mu$ the learning rate and $\\delta_{w}$ the local gradient at the output of a neuron that has $w$ as a parameter.\n",
    "\n",
    "#### Compute the cost function\n",
    "\n",
    "The cost function $\\xi$ used in this model is the same cross-entropy cost function used in part 2:\n",
    "\n",
    "$$\\xi(T,Y) = - \\sum_{i=1}^{n} \\left[ t_i log(y_i) + (1-t_i)log(1-y_i) \\right]$$\n",
    "\n",
    "\n",
    "#### Update the output layer\n",
    "\n",
    "At the output the gradient $\\delta_{w}$ is computed by ${\\partial \\xi}/{\\partial w}$. We worked out this formula for batch processing in part 2:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{oi}} = \\frac{\\partial z_{o}}{\\partial w_{oi}} \\frac{\\partial y}{\\partial z_{o}} \\frac{\\partial \\xi}{\\partial y} = \\sum_{j=1}^n h_{ji} (y_j - t_j)$$\n",
    "\n",
    "With $z_{o} = H * W_o$. We can write this formula out as matrix operations with the parameters of the output layer:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial W_o} = H^T \\cdot (Y-T) $$\n",
    "\n",
    "The resulting gradient is a $3 \\times 1$ [Jacobian matrix](http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant):\n",
    "\n",
    "$$J_o =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\xi}{\\partial b_o} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{o1}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{o2}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "#### Update the input layer\n",
    "\n",
    "At the hidden layer the gradient $\\delta_{w}$ of the neuron with parameter $w$ is computed the same way:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{hij}} = \\frac{\\partial z_{hj}}{\\partial w_{hij}} \\frac{\\partial h_j}{\\partial z_{hj}} \\frac{\\partial \\xi}{\\partial h_j}$$\n",
    "\n",
    "With for all samples $k$: $z_{hj} = \\sum_{i=0}^3 x_{ki} * w_{hij} $. And with ${\\partial \\xi}/{\\partial h_j}$ the gradient of the error at the output of the hidden node $h_j$ with respect to this $h_j$. This error can be interpreted as the contribution of $h_j$ to the total error.\n",
    "How do we define this error at the output of the hidden nodes? For each output neuron $h_j$ we can compute this as the sum of the errors propagated back from the connections going out of $h_j$:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial h_j} = \\frac{\\partial z_{o}}{\\partial h_j} \\frac{\\partial \\xi}{\\partial z_{o}} = w_{oj} \\frac{\\partial \\xi}{\\partial z_{o}}$$\n",
    "\n",
    "Because of this, and because ${\\partial z_{hj}}/{\\partial w_{hij}} = x_{i}$ and ${\\partial h_j}/{\\partial z_{hj}} = h_j * (1-h_j)$ and ${\\partial \\xi}/{\\partial z_{o}} = y - t = E$ we compute ${\\partial \\xi}/{\\partial w_{hij}}$ over all samples $k$ as:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{hij}} = \\sum_{k=1}^n x_{ki} * h_{kj} * (1-h_{kj}) * w_{oj}  * (y_k-t_k)$$\n",
    "\n",
    "This can be written in matrix format as:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial W_h} = X^T \\cdot [H \\circ (1-H) \\circ (Y - T) * W_o]$$\n",
    "\n",
    "With $\\circ$ the [elementwise product](http://en.wikipedia.org/wiki/Hadamard_product).\n",
    "\n",
    "The resulting gradient is a $3 \\times 2$ [Jacobian matrix](http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant):\n",
    "\n",
    "$$J_o =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\xi}{\\partial b_{h1}} & \\frac{\\partial \\xi}{\\partial b_{h2}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{h11}} & \\frac{\\partial \\xi}{\\partial w_{h12}} \\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{h21}} & \\frac{\\partial \\xi}{\\partial w_{h22}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "To start out the gradient descent algorithm, you typically start with picking the initial parameters at random and start updating these parameters in the direction of the negative gradient with help of the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "def cost(Y, T):\n",
    "    return - np.sum(np.multiply(T, np.log(Y)) + np.multiply((1-T), np.log(1-Y)))\n",
    "\n",
    "# Define the error function\n",
    "def error(Y, T):\n",
    "    return Y - T\n",
    "\n",
    "# define the gradient function for the output layer\n",
    "def gradient_output(H, E): \n",
    "    return  H.T * E\n",
    "\n",
    "def gradient_input(x, wh, bh, error_gradient):\n",
    "    return np.sum(x * error_gradient, axis=0)\n",
    "\n",
    "# # define the update function delta w which returns the \n",
    "# #  delta w for each weight in a vector\n",
    "# def delta_w(w_k, x, t, learning_rate):\n",
    "#     return learning_rate * gradient(w_k, x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = cost(Y, T)\n",
    "print 'C.shape: ', C.shape\n",
    "\n",
    "E = error(Y, T)\n",
    "print 'E.shape: ', E.shape\n",
    "\n",
    "print 'H.shape: ', H.shape\n",
    "Jo = gradient_output(H, E)\n",
    "print 'Jo.shape: ', Jo.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
