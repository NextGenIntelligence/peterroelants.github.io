{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Neural network\n",
    "## Part 3: Hidden layer\n",
    "\n",
    "This tutorial is part 3 of the previous tutorials on neural networks (TODO: url). While the previous tutorials described very simple single layer regression and classification models, this tutorial will describe a 2-class classification neural network with 1 input dimension, and a non-linear hidden layer with 2 dimensions. While we didn't add the bias parameters to the previous 2 models, we will add them to this model. The network of this model is shown in the following figure:\n",
    "\n",
    "![Image of the logistic model](https://dl.dropboxusercontent.com/u/8938051/Blog_images/SimpleANN03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset \n",
    "\n",
    "In this example the target classes $t$ corresponding to the inputs $x$ will be generated from 2 class distributions: blue ($t=1$) and red ($t=0$). Where the red class is a [multimodal distribution](http://en.wikipedia.org/wiki/Multimodal_distribution) that surrounds the distribution of the blue class. This results in a 1D dataset that is not linearly seperable. The model from part 2 won't be able to classify both classes correctly since it can learn only linear seperators. By adding a hidden layer with a non-linear transfer function the model will be able to train a non-linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and generate the samples\n",
    "nb_of_samples_per_class = 20  # The number of sample in each class\n",
    "blue_mean = [0]  # The mean of the blue class\n",
    "red_left_mean = [-2]  # The mean of the red class\n",
    "red_right_mean = [2]  # The mean of the red class\n",
    "\n",
    "std_dev = 0.5  # standard deviation of both classes\n",
    "# Generate samples from both classes\n",
    "x_blue = np.random.randn(nb_of_samples_per_class, 1) * std_dev + blue_mean\n",
    "x_red_left = np.random.randn(nb_of_samples_per_class/2, 1) * std_dev + red_left_mean\n",
    "x_red_right = np.random.randn(nb_of_samples_per_class/2, 1) * std_dev + red_right_mean\n",
    "\n",
    "\n",
    "# Merge samples in set of input variables x, and corresponding set of\n",
    "# output variables t\n",
    "x = np.vstack((x_blue, x_red_left, x_red_right))\n",
    "# print x\n",
    "t = np.vstack((np.ones((x_blue.shape[0],1)), \n",
    "               np.zeros((x_red_left.shape[0],1)), \n",
    "               np.zeros((x_red_right.shape[0], 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot samples from both classes as lines on a 1D space\n",
    "plt.figure(figsize=(8,0.5))\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-1,1)\n",
    "# Plot samples\n",
    "plt.plot(x_blue, np.zeros_like(x_blue), 'b|', ms = 30) \n",
    "plt.plot(x_red_left, np.zeros_like(x_red_left), 'r|', ms = 30) \n",
    "plt.plot(x_red_right, np.zeros_like(x_red_right), 'r|', ms = 30) \n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear transfer function \n",
    "\n",
    "The non-linear transfer function used in the hidden layer of this example is the [Gaussian](http://en.wikipedia.org/wiki/Gaussian_function) [radial basis function](http://en.wikipedia.org/wiki/Radial_basis_function) (RBF).  \n",
    "The RBF is a transfer function that is not usally used in neural networks, exept for [radial basis function networks](http://en.wikipedia.org/wiki/Radial_basis_function_network). One of the most common transfer functions in neural networks is the [sigmoid](http://en.wikipedia.org/wiki/Sigmoid_function) transfer function.\n",
    "The RBF will allow to seperate the blue samples from the red samples in this simple example by only activating for a certain region around the origin. The RBF is plotted in the figure below and is defined in this example as:\n",
    "\n",
    "$$ \\text{RBF} = \\phi(z) = e^{-z^2} $$\n",
    "\n",
    "The derivative of this RBF function is:\n",
    "\n",
    "$$ \\frac{d \\phi(z)}{dz} = -2 z e^{-z^2} = -2 z \\phi(z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the rbf function\n",
    "def rbf(z): return np.exp(-z**2)\n",
    "\n",
    "# Plot the rbf function\n",
    "z = np.linspace(-6,6,100)\n",
    "plt.plot(z, rbf(z), 'b-')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$e^{-z^2}$')\n",
    "plt.title('rbf function')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization by backpropagation\n",
    "\n",
    "We will train this model by using the [backpropagation](http://en.wikipedia.org/wiki/Backpropagation) algorithm that is typically used to train neural networks. Each step in the backpropagation algorithm consists of two steps:\n",
    "\n",
    "1. A forward propagation step to compute the output of the network.\n",
    "2. A backward propagation step in which the error at the end of the network is propagated backwards through all the neurons, while updating their parameters.\n",
    "\n",
    "### 1. Forward step\n",
    "\n",
    "During the forward step the input will be propagated layer by layer through the network to compute the final output of the network.\n",
    "\n",
    "#### Compute activations of hidden layer\n",
    "\n",
    "The activations $h$ of the hidden layer will be computed by:\n",
    "\n",
    "$$h = \\phi(x*w_h) = e^{-(x*w_h)^2} $$\n",
    "\n",
    "With $w_h$ the weight parameter that transforms the input before applying the RBF transfer function.\n",
    "\n",
    "#### Compute activations of output \n",
    "\n",
    "The output of the final layer and network will be computed by passing the hidden activations $h$ as input to the logistic output function:\n",
    "\n",
    "$$ y = \\sigma(h * w_o - 1) = \\frac{1}{1+e^{-h * w_o - 1}} $$\n",
    "\n",
    "With $w_o$ the weight parameter of the output layer.  \n",
    "Note that we add a bias (intercept) term of $-1$ to the input of the logistic output neuron. Remember from part 2 that the logistic output neuron can only learn a decision boundary that goes through the origin $(0)$. Since the RBF in the hidden layer projects all input variables to a range between $0$ and $+ \\infty$, the output layer without an intercept will not be able to learn any usefull classifier, because none of the samples will be below $0$ and thus lie on the left side of the decision boundary. By adding a bias term the decision boundary is moved from the intercept. Normally the value of this bias termed is learned together with the rest of the weight parameters, but to keep this model simple we just make this bias constant in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def logistic(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to compute the hidden activations\n",
    "def hidden_activations(x, wh):\n",
    "    return rbf(x * wh)\n",
    "\n",
    "# Define output layer feedforward\n",
    "def output_activations(h , wo):\n",
    "    return logistic(h * wo - 1)\n",
    "\n",
    "# Define the neural network function\n",
    "def nn(x, wh, wo): \n",
    "    return output_activations(hidden_activations(x, wh), wo)\n",
    "\n",
    "# Define the neural network prediction function that only returns\n",
    "#  1 or 0 depending on the predicted class\n",
    "def nn_predict(x, wh, wo): \n",
    "    return np.around(nn(x, wh, wo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Weights\n",
    "wh = 1\n",
    "wo = 1\n",
    "\n",
    "h = hidden_activations(x, wh)\n",
    "print 'h.shape: ', h.shape\n",
    "o = output_activations(h, wo)\n",
    "print 'o.shape: ', o.shape\n",
    "\n",
    "y = nn(x, wh, wo)\n",
    "print 'y.shape: ', y.shape\n",
    "\n",
    "y_pred = nn_predict(x, wh, wo)\n",
    "print 'y_pred.shape: ', y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Backward step\n",
    "\n",
    "The backward step will begin with computing the cost at the output node. This cost will then be propagated backwards layer by layer through the network to update the parameters.\n",
    "\n",
    "The [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) algorithm is used in every layer to update the parameters in the direction of the negative [gradient](http://en.wikipedia.org/wiki/Gradient).\n",
    "\n",
    "The parameters $w$ are updated by $w(k+1) = w(k) - \\Delta w(k+1)$. $\\Delta w$ is defined as: $\\Delta w = \\mu \\delta_{w}$ with $\\mu$ the learning rate and $\\delta_{w}$ the local gradient at the output of a neuron that has $w$ as a parameter.\n",
    "\n",
    "#### Compute the cost function\n",
    "\n",
    "The cost function $\\xi$ used in this model is the same cross-entropy cost function used in part 2 of these tutorials (TODO url):\n",
    "\n",
    "$$\\xi(t,y) = - \\sum_{i=1}^{n} \\left[ t_i log(y_i) + (1-t_i)log(1-y_i) \\right]$$\n",
    "\n",
    "This cost function is plotted for the $w_h$ and $w_o$ paramters in the next figure. Note that this error surface is not convex anymore, and that the $w_h$ parameter mirrors the cost function along the $w_h = 0$ axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "def cost(y, t):\n",
    "    return - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Plot the cost in function of the weights\n",
    "# Define a vector of weights for which we want to plot the cost\n",
    "nb_of_ws = 200 # compute the cost nb_of_ws times in each dimension\n",
    "wsh = np.linspace(-10, 10, num=nb_of_ws) # weight 1\n",
    "wso = np.linspace(-15, 15, num=nb_of_ws) # weight 2\n",
    "ws_x, ws_y = np.meshgrid(wsh, wso) # generate grid\n",
    "cost_ws = np.zeros((nb_of_ws, nb_of_ws)) # initialize cost matrix\n",
    "# Fill the cost matrix for each combination of weights\n",
    "for i in xrange(nb_of_ws):\n",
    "    for j in xrange(nb_of_ws):\n",
    "        cost_ws[i,j] = cost(nn(x, ws_x[i,j], ws_y[i,j]) , t)\n",
    "# Plot the cost function surface\n",
    "# plt.contourf(ws_x, ws_y, cost_ws, 20)\n",
    "# cbar = plt.colorbar()\n",
    "# cbar.ax.set_ylabel('cost')\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_surface(ws_x, ws_y, cost_ws, linewidth=0, cmap=cm.coolwarm)\n",
    "ax.view_init(elev=40, azim=-30)\n",
    "fig.colorbar(surf)\n",
    "plt.xlabel('$w_h$')\n",
    "plt.ylabel('$w_o$')\n",
    "plt.title('Cost function surface')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the output layer\n",
    "\n",
    "At the output the gradient ${\\partial \\xi}/{\\partial w_o}$ can be worked out the same way as we did in part 2:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_o} = \\frac{\\partial z_o}{\\partial w_o} \\frac{\\partial y}{\\partial z_o} \\frac{\\partial \\xi}{\\partial y} = h (y-t) = h * \\delta_{o}$$\n",
    "\n",
    "With $z_o = h * w_o$, and $\\delta_{o}$ the gradient of the error at the output of the neural network with respect to this output.\n",
    "\n",
    "\n",
    "#### Update the input layer\n",
    "\n",
    "At the hidden layer the gradient ${\\partial \\xi}/{\\partial w_{h}}$ of the hidden neuron is computed the same way:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{h}} = \\frac{\\partial z_{h}}{\\partial w_{h}} \\frac{\\partial h}{\\partial z_{h}} \\frac{\\partial \\xi}{\\partial h}$$\n",
    "\n",
    "With $z_{h} = x * w_{h} $. And with ${\\partial \\xi}/{\\partial h} = \\delta_{h}$ the gradient of the error at the output of the hidden neuron with respect to this $h$. This error can be interpreted as the contribution of $h$ to the final error.\n",
    "How do we define this error gradient $\\delta_{h}$ at the output of the hidden nodes? It can be computed as the error gradient propagated back from the connection going out of the neuron with output $h$.\n",
    "\n",
    "$$\\delta_{h} = \\frac{\\partial \\xi}{\\partial h} = \\frac{\\partial z_{o}}{\\partial h} \\frac{\\partial y}{\\partial z_{o}} \\frac{\\partial \\xi}{\\partial y} = w_{o} (y - t) = w_{o} \\delta_{o} $$\n",
    "\n",
    "Because of this, and because ${\\partial z_{h}}/{\\partial w_{h}} = x$ and ${\\partial h}/{\\partial z_{h}} = -2 z_h h$ we compute ${\\partial \\xi}/{\\partial w_{h}}$ as:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_{h}} = x * -2 z_h h * \\delta_{h}  $$\n",
    "\n",
    "The gradients for each parameter can again be summed up to compute the update for a batch of input examples.\n",
    "\n",
    "To start out the gradient descent algorithm, you typically start with picking the initial parameters at random and start updating these parameters in the direction of the negative gradient with help of the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the error function\n",
    "def gradient_output(y, t):\n",
    "    return y - t\n",
    "\n",
    "# Define the gradient function for the weight parameter at the output layer\n",
    "def gradient_weight_out(h, grad_output): \n",
    "    return  h * grad_output\n",
    "\n",
    "# Define the gradient function for the hidden layer\n",
    "def gradient_hidden(wo, grad_output):\n",
    "    return wo * grad_output\n",
    "\n",
    "# Define the gradient function for the weight parameter at the hidden layer\n",
    "def gradient_weight_hidden(x, zh, h, grad_hidden):\n",
    "    return x * -2 * zh * h * grad_hidden\n",
    "\n",
    "# Define the update function to update the network parameters over 1 iteration\n",
    "def backprop_update(x, t, wh, wo, learning_rate):\n",
    "    # Compute the output of the network\n",
    "    # This can be done with y = nn(x, wh, wo), but we need the intermediate \n",
    "    #  h and zh for the weight updates.\n",
    "    zh = x * wh\n",
    "    h = rbf(zh)  # hidden_activations(x, wh)\n",
    "    y = output_activations(h, wo)\n",
    "    # Compute the gradient at the output\n",
    "    grad_output = gradient_output(y, t)\n",
    "    # Get the delta for wo\n",
    "    d_wo = learning_rate * gradient_weight_out(h, grad_output)\n",
    "    # Compute the gradient at the hidden layer\n",
    "    grad_hidden = gradient_hidden(wo, grad_output)\n",
    "    # Get the delta for wh\n",
    "    d_wh = learning_rate * gradient_weight_hidden(x, zh, h, grad_hidden)\n",
    "    # return the update parameters\n",
    "    return (wh-d_wh.sum(), wo-d_wo.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run backpropagation\n",
    "# Set the initial weight parameter\n",
    "wh = 2\n",
    "wo = -5\n",
    "# Set the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Plot the error surface\n",
    "# plt.contourf(ws_x, ws_y, cost_ws, 20)\n",
    "# cbar = plt.colorbar()\n",
    "# cbar.ax.set_ylabel('cost')\n",
    "# plt.title('Backpropagation updates')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_surface(ws_x, ws_y, cost_ws, linewidth=0, cmap=cm.coolwarm)\n",
    "ax.view_init(elev=60, azim=-30)\n",
    "cbar = fig.colorbar(surf)\n",
    "cbar.ax.set_ylabel('cost')\n",
    "\n",
    "def c_inp(x, wh, wo, t):\n",
    "    return cost(nn(x, wh, wo) , t)\n",
    "\n",
    "# Start the gradient descent updates and plot the iterations\n",
    "nb_of_iterations = 60  # number of gradient descent updates\n",
    "lr_update = 1 - (1.0/100)\n",
    "for i in xrange(nb_of_iterations):\n",
    "    learning_rate *= lr_update\n",
    "#     print learning_rate\n",
    "    # Plot the weight-cost value and the line that represents the update\n",
    "    ax.plot([wh], [wo], [c_inp(x, wh, wo, t)], 'ko')  # Plot the weight cost value\n",
    "    w_new = backprop_update(x, t, wh, wo, learning_rate) # update the weights\n",
    "#     print w_new\n",
    "    ax.plot([wh, w_new[0]], [wo, w_new[1]], [c_inp(x, wh, wo, t), c_inp(x, w_new[0], w_new[1], t)], 'k-')\n",
    "#     plt.text(wh-0.2, wo+0.4, '$w({})$'.format(i))\n",
    "    wh, wo = w_new  # set the weight to the updated weights\n",
    "    \n",
    "# Plot the last weight, axis, and show figure\n",
    "ax.plot([wh], [wo], [c_inp(x, wh, wo, t)], 'ko')\n",
    "# plt.text(wh-0.2, wo+0.4, '$w({})$'.format(nb_of_iterations))  \n",
    "ax.set_xlabel('$w_h$')\n",
    "ax.set_ylabel('$w_o$')\n",
    "ax.set_zlabel('cost')\n",
    "plt.title('Gradient descent updates on cost surface')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot samples from both classes as lines on a 1D space\n",
    "plt.figure(figsize=(8,0.5))\n",
    "plt.xlim(-0.01,1)\n",
    "plt.ylim(-1,1)\n",
    "# print hidden_activations(x, wh)\n",
    "# Plot samples\n",
    "plt.plot(hidden_activations(x_blue, wh), np.zeros_like(x_blue), 'b|', ms = 30) \n",
    "plt.plot(hidden_activations(x_red_left, wh), np.zeros_like(x_red_left), 'r|', ms = 30) \n",
    "plt.plot(hidden_activations(x_red_right, wh), np.zeros_like(x_red_right), 'r|', ms = 30) \n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "print nn(x, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a color map to show the classification colors of each grid point\n",
    "cmap = ListedColormap([\n",
    "        colorConverter.to_rgba('r', alpha=0.40),\n",
    "        colorConverter.to_rgba('b', alpha=0.40)])\n",
    "\n",
    "nb_of_xs = 10\n",
    "xs = np.linspace(-4, 4, num=nb_of_xs)\n",
    "ys = np.linspace(-1, 1, num=nb_of_xs)\n",
    "xx, yy = np.meshgrid(xs, ys) # create the grid\n",
    "# Initialize and fill the classification plane\n",
    "classification_plane = np.zeros((nb_of_xs, nb_of_xs))\n",
    "for i in xrange(nb_of_xs):\n",
    "    for j in xrange(nb_of_xs):\n",
    "#         classification_plane[i,j] = rbf(xx[i,j] * 3)\n",
    "        classification_plane[i,j] = nn_predict(xx[i,j], wh, wo)\n",
    "print classification_plane\n",
    "# Create a color map to show the classification colors of each grid point\n",
    "cmap = ListedColormap([\n",
    "        colorConverter.to_rgba('r', alpha=0.25),\n",
    "        colorConverter.to_rgba('b', alpha=0.25)])\n",
    "\n",
    "# Plot the classification plane with decision boundary and input samples\n",
    "\n",
    "\n",
    "# Plot samples from both classes as lines on a 1D space\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.contourf(xx, yy, classification_plane, cmap=cmap)\n",
    "plt.colorbar()\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-1,1)\n",
    "# Plot samples\n",
    "plt.plot(x_blue, np.zeros_like(x_blue), 'b|', ms = 30) \n",
    "plt.plot(x_red_left, np.zeros_like(x_red_left), 'r|', ms = 30) \n",
    "plt.plot(x_red_right, np.zeros_like(x_red_right), 'r|', ms = 30) \n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
