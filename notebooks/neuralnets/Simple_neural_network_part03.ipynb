{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Neural network\n",
    "## Part 3: Hidden layer\n",
    "\n",
    "This tutorial is part 3 of the previous tutorials on neural networks (TODO: url). While the previous tutorials described very simple single layer regression and classification models, this tutorial will describe a 2-class classification neural network with 1 input dimension, and a non-linear hidden layer with 2 dimensions. While we didn't add the bias parameters to the previous 2 models, we will add them to this model. The network of this model is shown in the following figure:\n",
    "\n",
    "![Image of the logistic model](https://dl.dropboxusercontent.com/u/8938051/Blog_images/SimpleANN03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset \n",
    "\n",
    "In this example the target classes $t$ will be generated from 2 class distributions: blue ($t=1$) and red ($t=0$). Where the red class is a [multimodal distribution](http://en.wikipedia.org/wiki/Multimodal_distribution) that surrounds the distribution of the blue class. This results in a 1D dataset that is not linearly seperable. The model from part 2 won't be able to classify both classes correctly since it can learn only linear seperators. By adding a hidden layer with a non-linear transfer function the model will be able to train a non-linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and generate the samples\n",
    "nb_of_samples_per_class = 20  # The number of sample in each class\n",
    "blue_mean = [0]  # The mean of the blue class\n",
    "red_left_mean = [-2]  # The mean of the red class\n",
    "red_right_mean = [2]  # The mean of the red class\n",
    "\n",
    "std_dev = 0.5  # standard deviation of both classes\n",
    "# Generate samples from both classes\n",
    "x_blue = np.random.randn(nb_of_samples_per_class, 1) * std_dev + blue_mean\n",
    "x_red_left = np.random.randn(nb_of_samples_per_class/2, 1) * std_dev + red_left_mean\n",
    "x_red_right = np.random.randn(nb_of_samples_per_class/2, 1) * std_dev + red_right_mean\n",
    "\n",
    "\n",
    "# Merge samples in set of input variables x, and corresponding set of\n",
    "# output variables t\n",
    "x = np.vstack((x_blue, x_red_left, x_red_right))\n",
    "# print x\n",
    "t = np.vstack((np.ones((x_blue.shape[0],1)), \n",
    "               np.zeros((x_red_left.shape[0],1)), \n",
    "               np.zeros((x_red_right.shape[0], 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot samples from both classes as lines on a 1D space\n",
    "plt.figure(figsize=(8,0.5))\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-1,1)\n",
    "# Plot samples\n",
    "plt.plot(x_blue, np.zeros_like(x_blue), 'b|', ms = 30) \n",
    "plt.plot(x_red_left, np.zeros_like(x_red_left), 'r|', ms = 30) \n",
    "plt.plot(x_red_right, np.zeros_like(x_red_right), 'r|', ms = 30) \n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear transfer function \n",
    "\n",
    "The non-linear transfer function used in the hidden layer of this example is the [Gaussian](http://en.wikipedia.org/wiki/Gaussian_function) [radial basis function](http://en.wikipedia.org/wiki/Radial_basis_function) (RBF).  \n",
    "The RBF is a transfer function that is not usally used in neural networks, exept for [radial basis function networks](http://en.wikipedia.org/wiki/Radial_basis_function_network). One of the most common transfer functions in neural networks is the [sigmoid](http://en.wikipedia.org/wiki/Sigmoid_function) transfer function.\n",
    "The RBF will allow to seperate the blue samples from the red samples in this simple example by only activating for a certain region around the origin. The RBF is plotted in the figure below and is defined in this example as:\n",
    "\n",
    "$$ \\text{RBF} = \\phi(z) = e^{-z^2} $$\n",
    "\n",
    "The derivative of this RBF function is:\n",
    "\n",
    "$$ \\frac{d \\phi(z)}{dz} = -2 z e^{-z^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the rbf function\n",
    "def rbf(z): return np.exp(-z**2)\n",
    "\n",
    "# Plot the rbf function\n",
    "z = np.linspace(-6,6,100)\n",
    "plt.plot(z, rbf(z), 'b-')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$e^{-z^2}$')\n",
    "plt.title('rbf function')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization by backpropagation\n",
    "\n",
    "We will train this model by using the [backpropagation](http://en.wikipedia.org/wiki/Backpropagation) algorithm that is typically used to train neural networks. Each step in the backpropagation algorithm consists of two steps:\n",
    "\n",
    "1. A forward propagation step to compute the output of the network.\n",
    "2. A backward propagation step in which the error at the end of the network is propagated backwards through all the neurons, while updating their parameters.\n",
    "\n",
    "### 1. Forward step\n",
    "\n",
    "During the forward step the input will be propagated layer by layer through the network to compute the final output of the network.\n",
    "\n",
    "The $n$ input samples with $1$ variable each are given as a $n \\times 1$ matrix $X = [x_1 \\ldots x_n]^T$. These inputs are projected onto the 2 dimension of the hidden layer $H$ by according to: \n",
    "\n",
    "$$H = \\sigma(X * W_h + b_h) = \\frac{1}{1+e^{-(X * W_h + b_h)}} $$\n",
    "\n",
    "Where $W_h = [w_{h1}, w{h2}]$ the weight matrix, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def logistic(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to compute the hidden activations\n",
    "def hidden_activations(x, wh):\n",
    "    return rbf(x * wh)\n",
    "\n",
    "# Define output layer feedforward\n",
    "def output_activations(h , wo):\n",
    "    return logistic(h * wo)\n",
    "\n",
    "# Define the neural network function\n",
    "def nn(x, wh, wo): \n",
    "    return output_activations(hidden_activations(x, wh), wo)\n",
    "\n",
    "# Define the neural network prediction function that only returns\n",
    "#  1 or 0 depending on the predicted class\n",
    "def nn_predict(x, wh, wo): \n",
    "    return np.around(nn(x, wh, wo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weights\n",
    "wh = 1\n",
    "wo = 1\n",
    "\n",
    "h = hidden_activations(x, wh)\n",
    "print 'h.shape: ', h.shape\n",
    "o = output_activations(h, wo)\n",
    "print 'o.shape: ', o.shape\n",
    "\n",
    "y = nn(x, wh, wo)\n",
    "print 'y.shape: ', y.shape\n",
    "\n",
    "y_pred = nn_predict(x, wh, wo)\n",
    "print 'y_pred.shape: ', y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "def cost(y, t):\n",
    "    return - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the cost in function of the weights\n",
    "# Define a vector of weights for which we want to plot the cost\n",
    "nb_of_ws = 100 # compute the cost nb_of_ws times in each dimension\n",
    "wsh = np.linspace(-10, 10, num=nb_of_ws) # weight 1\n",
    "wso = np.linspace(-10, 10, num=nb_of_ws) # weight 2\n",
    "ws_x, ws_y = np.meshgrid(wsh, wso) # generate grid\n",
    "cost_ws = np.zeros((nb_of_ws, nb_of_ws)) # initialize cost matrix\n",
    "# Fill the cost matrix for each combination of weights\n",
    "for i in xrange(nb_of_ws):\n",
    "    for j in xrange(nb_of_ws):\n",
    "        cost_ws[i,j] = cost(nn(x, ws_x[i,j], ws_y[i,j]) , t)\n",
    "# Plot the cost function surface\n",
    "plt.contourf(ws_x, ws_y, cost_ws, 20)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('cost')\n",
    "plt.xlabel('$w_h$')\n",
    "plt.ylabel('$w_o$')\n",
    "plt.title('Cost function surface')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Weights and biases\n",
    "wh = np.asmatrix([[1, 1]])\n",
    "bh = np.asmatrix([[0, 0]])\n",
    "wo = np.asmatrix([[1], [1]])\n",
    "bo = np.asmatrix([[0]])\n",
    "\n",
    "\n",
    "# Define hidden feedforward\n",
    "def layer_hidden_ff(x, wh, bh):\n",
    "    return logistic(x.dot(wh)+bh)\n",
    "\n",
    "# print layer_hidden_ff(x, wh, bh)\n",
    "\n",
    "# Define output layer feedforward\n",
    "def layer_output_ff(h, wo, bo):\n",
    "    return logistic(h.dot(wo) + bo)\n",
    "\n",
    "# print layer_output_ff(layer_hidden_ff(x, wh, bh), wo, bo)\n",
    "\n",
    "# Define the neural network function\n",
    "def nn(x, wh, bh, wo, bo): \n",
    "    return layer_output_ff(layer_hidden_ff(x, wh, bh), wo, bo)\n",
    "\n",
    "# Define the neural network prediction function that only returns\n",
    "#  1 or 0 depending on the predicted class\n",
    "def nn_predict(x, wh, bh, wo, bo): return np.around(nn(x, wh, bh, wo, bo))\n",
    "\n",
    "# print nn(x, wh, bh, wo, bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "def cost(y, t):\n",
    "    return - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))\n",
    "\n",
    "y = nn(x, wh, bh, wo, bo)\n",
    "print cost(y, t)\n",
    "\n",
    "h = layer_hidden_ff(x, wh, bh)\n",
    "\n",
    "# print layer_output_ff(h, wo, bo)\n",
    "# print t\n",
    "# print h\n",
    "\n",
    "# define the gradient function for the output layer\n",
    "def gradient_output(h, wo, bo, t): \n",
    "    return (layer_output_ff(h, wo, bo) - t).T * h\n",
    "\n",
    "go = gradient_output(h, wo, bo, t)\n",
    "print go\n",
    "\n",
    "def gradient_input(x, wh, bh, error_gradient):\n",
    "    return np.sum(x * error_gradient, axis=0)\n",
    "\n",
    "print gradient_input(x, wh, bh, go)\n",
    "\n",
    "# define the update function delta w which returns the \n",
    "#  delta w for each weight in a vector\n",
    "def delta_w(w_k, x, t, learning_rate):\n",
    "    return learning_rate * gradient(w_k, x, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
