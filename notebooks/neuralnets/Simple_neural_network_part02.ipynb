{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A Simple Neural network\n",
    "## Part 2: Logistic regression (classification)\n",
    "\n",
    "This tutorial is part 2 of the [previous tutorial on neural networks](http://peterroelants.github.io/posts/2015/05/18/Simple_neural_network_part01/). While the previous tutorial described a very simple one-input-one-output linear regression model, this model will describe a 2-class classification neural network with two input dimensions. This model is known in statistics as the [logistic regression](http://en.wikipedia.org/wiki/Logistic_regression) model.\n",
    "\n",
    "![Image of the logistic model](https://dl.dropboxusercontent.com/u/8938051/Blog_images/SimpleANN02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "from mpl_toolkits.mplot3d import Axes3D # 3D plotting\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the class distributions\n",
    "\n",
    "In this example the target classes $t$ will be generated from 2 class distributions: blue ($t=1$) and red ($t=0$). Samples from both classes are sampled from their respective distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_of_samples_per_class = 20  # The number of sample in each class\n",
    "red_mean = [-1,0]  # The mean of the red class\n",
    "blue_mean = [1,0]  # The mean of the blue class\n",
    "std_dev = 1.2  # standard deviation of both classes\n",
    "# Generate samples from both classes\n",
    "x_red = np.random.randn(nb_of_samples_per_class, 2) * std_dev + red_mean\n",
    "x_blue = np.random.randn(nb_of_samples_per_class, 2) * std_dev + blue_mean\n",
    "\n",
    "x = np.vstack((x_red, x_blue))\n",
    "t = np.vstack((np.zeros((nb_of_samples_per_class,1)), np.ones((nb_of_samples_per_class,1))))\n",
    "print t\n",
    "# print x\n",
    "# print x_red\n",
    "# print x_blue\n",
    "# Plot both classon on the x1, x2 plane\n",
    "plt.plot(x_red[:,0], x_red[:,1], 'ro', label='class red')\n",
    "plt.plot(x_blue[:,0], x_blue[:,1], 'bo', label='class blue')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.axis([-4, 4, -4, 4])\n",
    "plt.title('red vs blue classes in the input space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic function\n",
    "\n",
    "The goal to predict the target class $t$ from the input values $x$. The network is defined as having an input $x = [x_1, x_2]$ which gets transformed by the weights $w = [w_1, w_2]$ to generate the probability that sample $x$ belongs to class $t=1$. This probability $P(t=1| x,w)$ is represented by the output $y$ of the network computed as $y = \\sigma(x * w^T)$. $\\sigma$ is the [logistic function](http://en.wikipedia.org/wiki/Logistic_function) and is defined as:\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "This logistic function maps the input $z$ to an output between $0$ and $1$ as is illustrated in the figure below.\n",
    "\n",
    "We can write the probabilities that the class is $t=1$ or $t=0$ given input $x$ as:\n",
    "\n",
    "$$ P(t=1| x) = \\sigma(x * w^T) = \\frac{1}{1+e^{-x * w^T}} $$\n",
    "$$ P(t=0| x) = 1 - \\sigma(x * w^T) = \\frac{e^{-x * w^T}}{1+e^{-x * w^T}} $$\n",
    "\n",
    "Note that the logistic function is derived from the log [odds ratio](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm) of $P(t=1|x)$ over $P(t=0|x)$.\n",
    "\n",
    "$$ log \\frac{P(t=1|x)}{P(t=0|x)} = log \\frac{\\frac{1}{1+e^{-x * w^T}}}{\\frac{e^{-x * w^T}}{1+e^{-x * w^T}}} = log \\frac{1}{e^{-x * w^T}} = $$\n",
    "$$log(1) - log(e^{-x * w^T}) = x * w^T$$\n",
    "\n",
    "This means that the logg odds ratio $log(P(t=1|x)/P(t=0|x))$ changes linearly with the parameters $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def logistic(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Plot the logistic function\n",
    "z = np.linspace(-6,6,100)\n",
    "plt.plot(z, logistic(z), 'b-')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\sigma(z)$')\n",
    "plt.title('logistic function')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the cost function\n",
    "\n",
    "As written before the output of the model $y = \\sigma(x * w^T)$ can be interpreted as a probability $y$ that sample $x$ belongs to one class $(t=1)$, or probability $1-y$ that $x$ belongs to the other class $(t=0)$ in a two class classification problem. We note this down as: $P(t=1| x; w) = \\sigma(x * w^T) = y$.\n",
    "\n",
    "This model will be optimized by maximizing the [likelihood](http://en.wikipedia.org/wiki/Likelihood_function) that a given set of parameters $w$ can predict the correct class given an input set $x$ of size $n$, and corresponding labels $t$:\n",
    "\n",
    "$$\\underset{w}{\\text{argmax}}\\; \\mathcal{L}(w|t,x) = \\underset{w}{\\text{argmax}} \\prod_{i=1}^{n} \\mathcal{L}(w|t_i,x_i)$$\n",
    "\n",
    "We can rewrite the likelihood $\\mathcal{L}(w|t,x)$ as the [joint probability](http://en.wikipedia.org/wiki/Joint_probability_distribution) of generating $t$ and $x$ given the parameters $w$: $P(t,x|w)$. Since $P(A,B) = P(A|B)*P(B)$ this can be written as:\n",
    "\n",
    "$$P(t,x|w) = P(t|x;w)P(x|w)$$\n",
    "\n",
    "Since we are not interested in the probability of $x$, and $x$ is independent of $w$ we can reduce this to: $\\mathcal{L}(w|t,x) = P(t|x;w) = \\prod_{i=1}^{n} P(t_i|x_i;w)$. \n",
    "Since $t_i$ is a [Bernoulli variable](http://en.wikipedia.org/wiki/Bernoulli_distribution) we can rewrite this as: \n",
    "\n",
    "$$\\begin{split}\n",
    "P(t|x;w) & = \\prod_{i=1}^{n} P(t_i=1|x_i;w)^{t_i} * (1 - P(t_i=1|x_i;w))^{1-t_i} \\\\\n",
    "& = \\prod_{i=1}^{n} y_i^{t_i} * (1 - y_i)^{1-t_i} \\end{split}$$\n",
    "\n",
    "Since the logartimic function is a monotone increasing function we can optimize the log-likelihood function $\\underset{w}{\\text{argmax}}\\; log \\mathcal{L}(w|t,x)$. This maximum will be the same as the maximum from the regular likelihood function. The log-likelihood function can be written as:\n",
    "\n",
    "$$\\begin{split} log \\mathcal{L}(w|t,x) & = log \\prod_{i=1}^{n} y_i^{t_i} * (1 - y_i)^{1-t_i} \\\\\n",
    "& = \\sum_{i=1}^{n} t_i log(y_i) + (1-t_i) log(1 - y_i)\n",
    "\\end{split}$$\n",
    "\n",
    "Minimizing the negative of this function (minimizing the negative log likelihood) corresponds to maximizing the likelihood. This error function $\\xi(t,y)$ is typically known as the [cross-entropy error function](http://en.wikipedia.org/wiki/Cross_entropy) (also known as log-loss):\n",
    "\n",
    "$$\\begin{split}\n",
    "\\xi(t,y) & = - log \\mathcal{L}(w|t,x) \\\\\n",
    "& = - \\sum_{i=1}^{n} \\left[ t_i log(y_i) + (1-t_i)log(1-y_i) \\right] \\\\\n",
    "& = - \\sum_{i=1}^{n} \\left[ t_i log(\\sigma((x_i * w) + b)) + (1-t_i)log(1-\\sigma((x_i * w) + b)) \\right]\n",
    "\\end{split}$$\n",
    "\n",
    "This function looks complicated but besided the previous derivation there are a couple of intuitions why this function is used as a cost function for logistic regression. First of all it can be rewritten as:\n",
    "\n",
    "$$ \\xi(t_i,y_i) = \n",
    "   \\begin{cases}\n",
    "   -log(y_i) & \\text{if } t_i = 1 \\\\\n",
    "   -log(1-y_i) & \\text{if } t_i = 0\n",
    "  \\end{cases}$$\n",
    "  \n",
    "Which in the case of $t_i=1$ is $0$ if $y_i=1$ $(-log(1)=0)$ and goes to infinity as $y_i \\rightarrow 0$ $(\\underset{y \\rightarrow 0}{\\text{lim}}  -log(y) = +\\infty)$. The reverse effect is happening if $t_i=0$.  \n",
    "So what we end up with is a cost function that is $0$ if the probability to predict the correct class is $1$, and goes to infinity as the probability to predict the correct class goes to $0$.\n",
    "\n",
    "Notice that the cost function $\\xi(t_i,y_i)$ is equal to the negative [log probability](http://en.wikipedia.org/wiki/Log_probability) that $x_i$ is classified as its correct class:  \n",
    "$-log(P(t_i=1| x_i,w,b)) = -log(y_i)$,  \n",
    "$-log(P(t_i=0| x_i,w,b)) =$ $-log(1-y_i)$.\n",
    "\n",
    "By minimizing the negative log probability we will maximize the log probability.\n",
    "\n",
    "Note that since $t_i$ can only be $0$ or $1$, we can write $\\xi(t_i,y_i)$ as:\n",
    "$$ \\xi(t_i,y_i) = -t_i log(y_i) - (1-t_i)log(1-y_i) $$\n",
    "\n",
    "Which will give $\\xi(t,y) = - \\sum_{i=1}^{n} \\left[ t_i log(y_i) + (1-t_i)log(1-y_i) \\right]$ if we sum over all $n$ samples.\n",
    "\n",
    "\n",
    "\n",
    "Another reason to use the cross-entropy function is that in simple logistic regression this results in a [convex](http://en.wikipedia.org/wiki/Convex_function) cost function, of which the global minimum will be easy to find. Note that this is not necessaraly the case anymore in multilayer neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.seterr(all='ignore')\n",
    "\n",
    "# Define the neural network function y = 1 / (1 + numpy.exp(-x*w))\n",
    "def nn(x, w): return logistic(x.dot(w.T))\n",
    "\n",
    "# Define the neural network prediction function that only returns\n",
    "#  1 or 0 depending on the predicted class\n",
    "def nn_predict(x,w): return np.around(nn(x,w))\n",
    "\n",
    "# define the cost function for a single sample\n",
    "def cost_sample(yi, ti):\n",
    "    if ti > 0.5:\n",
    "        return np.log(yi)\n",
    "    else: \n",
    "        return np.log(1-yi)\n",
    "    \n",
    "print np.multiply(np.matrix([[0],[0]]), np.matrix([[0],[0]]))\n",
    "# Define the cost function\n",
    "def cost(y, t):\n",
    "#     return - (t,np.log(y)) + np.multiply((1-t),np.log(1-y))).sum()\n",
    "    return - np.sum([cost_sample(yi, ti) for yi, ti in zip(y,t)])\n",
    "\n",
    "print cost(np.matrix([[0],[0]]), np.matrix([[0],[0]]))\n",
    "\n",
    "# # Define a vector of weights for which we want to plot the cost\n",
    "# ws = numpy.linspace(0, 4, num=100)  # weight values\n",
    "# cost_ws = numpy.vectorize(lambda w: cost(nn(x, w) , t))(ws)  # cost for each weight in ws\n",
    "\n",
    "nb_of_ws = 100\n",
    "ws1 = np.linspace(-5, 5, num=nb_of_ws)\n",
    "ws2 = np.linspace(-5, 5, num=nb_of_ws)\n",
    "# ws = np.mgrid[0:5,0:5]\n",
    "# ws_x, ws_y = np.meshgrid(ws1, ws2)\n",
    "ws_x = np.zeros((nb_of_ws, nb_of_ws))\n",
    "ws_y = np.zeros((nb_of_ws, nb_of_ws))\n",
    "# ws = numpy.array([numpy.linspace(i,j,5) for i,j in zip(ws1,ws2)])\n",
    "# print ws_x[19,19], ws_y[19,19]\n",
    "cost_ws = np.zeros((nb_of_ws, nb_of_ws))\n",
    "print cost_ws.shape\n",
    "for i in xrange(nb_of_ws):\n",
    "    for j in xrange(nb_of_ws):\n",
    "#         print np.asmatrix([ws1[i], ws2[j]])\n",
    "#         print nn(x, np.asmatrix([ws1[i], ws2[j]]))\n",
    "#         cost_ws[i,j] = cost(nn(x, np.asmatrix([ws_x[i,j], ws_y[i,j]])) , t)\n",
    "        ws_x[i,j] = ws1[i]\n",
    "        ws_y[i,j] = ws2[j]\n",
    "        cost_ws[i,j] = cost(nn(x, np.asmatrix([ws_x[i,j], ws_y[i,j]])) , t)\n",
    "#         print ws_x[i,j], ws_y[i,j], cost_ws[i,j]\n",
    "# print cost_ws\n",
    "\n",
    "# print cost_ws\n",
    "# fig = plt.figure()\n",
    "# ax = fig.gca(projection='3d')\n",
    "plt.contourf(ws_x, ws_y, cost_ws, 20)\n",
    "# surf = ax.plot_surface(ws_x, ws_y, cost_ws, cmap=cm.coolwarm)\n",
    "plt.colorbar()\n",
    "# ax.view_init(elev=25, azim=-50)\n",
    "# plt.imshow(cost_ws)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent optimization of the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import colorConverter, ListedColormap\n",
    "\n",
    "nb_of_xs = 100\n",
    "xs1 = np.linspace(-4, 4, num=nb_of_xs)\n",
    "xs2 = np.linspace(-4, 4, num=nb_of_xs)\n",
    "xx, yy = np.meshgrid(xs1, xs2)\n",
    "\n",
    "im = np.zeros((nb_of_xs, nb_of_xs))\n",
    "w = np.asmatrix([1,2])\n",
    "for i in xrange(nb_of_xs):\n",
    "    for j in xrange(nb_of_xs):\n",
    "        im[i,j] = nn_predict(np.asmatrix([xx[i,j], yy[i,j]]) , w)\n",
    "        \n",
    "print im\n",
    "\n",
    "cmap = ListedColormap([\n",
    "        colorConverter.to_rgba('r', alpha=0.40),\n",
    "        colorConverter.to_rgba('b', alpha=0.40)])\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "\n",
    "plt.contourf(xx, yy, im, cmap=cmap)\n",
    "\n",
    "\n",
    "\n",
    "# implot = plt.imshow(im)\n",
    "# cost_ws[i,j] = cost(nn(x, np.asmatrix([ws_x[i,j], ws_y[i,j]])) , t)\n",
    "\n",
    "plt.plot(x_red[:,0], x_red[:,1], 'ro', label='target red')\n",
    "plt.plot(x_blue[:,0], x_blue[:,1], 'bo', label='target blue')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('red vs blue classification boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import colorConverter\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "X = np.arange(-5, 5, 0.25)\n",
    "print X.shape\n",
    "Y = np.arange(-5, 5, 0.25)\n",
    "print Y.shape\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "print X.shape\n",
    "print Y.shape\n",
    "print X, Y\n",
    "R = np.sqrt(X**2 + Y**2)\n",
    "Z = np.sin(R)\n",
    "surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "        linewidth=0, antialiased=False)\n",
    "ax.set_zlim(-1.01, 1.01)\n",
    "\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.log(0) * 0\n",
    "np.isclose(0.99999999999999999999, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
