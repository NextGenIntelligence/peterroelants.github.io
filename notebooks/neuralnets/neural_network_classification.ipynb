{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network classification function\n",
    "\n",
    "If we want to do classification with neural networks we want to output a probability distribution over the classes from the output targets $t$. For the classification of 2 classes $t=1$ or $t=0$ we can use the [logistic function](http://en.wikipedia.org/wiki/Logistic_function) used in [logistic regression](http://en.wikipedia.org/wiki/Logistic_regression). For multiclass classification there exists an extention of this logistic function called the [softmax function](http://en.wikipedia.org/wiki/Softmax_function) which is used in [multinomial logistic regression](http://en.wikipedia.org/wiki/Multinomial_logistic_regression). The following sections will first explain the logistic function and how to optimise it, and hereafter it will explain the softmax function and how to derive it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D plots\n",
    "from matplotlib import cm # Colormaps\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic function\n",
    "\n",
    "The goal is to predict the target class $t$ from an input $z$. The probability $P(t=1 | z)$ that input $z$ is classified as class $t=1$ is represented by the output $y$ of the logistic function computed as $y = \\sigma(z)$. $\\sigma$ is the [logistic function](http://en.wikipedia.org/wiki/Logistic_function) and is defined as:\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "This logistic function maps the input $z$ to an output between $0$ and $1$ as is illustrated in the figure below.\n",
    "\n",
    "We can write the probabilities that the class is $t=1$ or $t=0$ given input $z$ as:\n",
    "\n",
    "$$ P(t=1| z) = \\sigma(z) = \\frac{1}{1+e^{-z}} $$\n",
    "$$ P(t=0| z) = 1 - \\sigma(z) = \\frac{e^{-z}}{1+e^{-z}} $$\n",
    "\n",
    "Note that the logistic function is derived from the log [odds ratio](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm) of $P(t=1|z)$ over $P(t=0|z)$.\n",
    "\n",
    "$$\\begin{split}\n",
    "log \\frac{P(t=1|z)}{P(t=0|z)} & = log \\frac{\\frac{1}{1+e^{-z}}}{\\frac{e^{-z}}{1+e^{-z}}} = log \\frac{1}{e^{-z}} \\\\\n",
    "& = log(1) - log(e^{-z}) = z\n",
    "\\end{split}$$\n",
    "\n",
    "This means that the logg odds ratio $log(P(t=1|z)/P(t=0|z))$ changes linearly with $z$. And if $z = x*w$ as in neural networks, this means that  the logg odds ratio changes linearly with the parameters $w$ and input samples $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def logistic(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Plot the logistic function\n",
    "z = np.linspace(-6,6,100)\n",
    "plt.plot(z, logistic(z), 'b-')\n",
    "plt.xlabel('$z$', fontsize=15)\n",
    "plt.ylabel('$\\sigma(z)$', fontsize=15)\n",
    "plt.title('logistic function')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of the logistic function\n",
    "\n",
    "Since neural networks typically use [gradient](http://en.wikipedia.org/wiki/Gradient) based opimization techniques such as [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) it is important to define the [derivative](http://en.wikipedia.org/wiki/Derivative) of the output $y$ of the logistic function with respect to its input $z$. ${\\partial y}/{\\partial z}$ can be calculated as:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial z} = \\frac{\\partial \\sigma(z)}{\\partial z} = \\frac{\\partial \\frac{1}{1+e^{-z}}}{\\partial z} = \\frac{-1}{(1+e^{-z})^2} *e^{-z}*-1 = \\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}}$$\n",
    "\n",
    "And since $1 - \\sigma(z)) = 1 - {1}/(1+e^{-z}) = {e^{-z}}/(1+e^{-z})$ this can be rewritten as:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial z} = \\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}} = \\sigma(z) * (1- \\sigma(z)) =  y (1-y)$$\n",
    "\n",
    "This derivative is plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def logistic_derivative(z): return logistic(z) * (1 - logistic(z))\n",
    "\n",
    "# Plot the logistic function\n",
    "z = np.linspace(-6,6,100)\n",
    "plt.plot(z, logistic_derivative(z), 'r-')\n",
    "plt.xlabel('$z$', fontsize=15)\n",
    "plt.ylabel('$\\\\frac{\\\\partial \\\\sigma(z)}{\\\\partial z}$', fontsize=15)\n",
    "plt.title('derivative of the logistic function')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy cost function for the logistic function\n",
    "\n",
    "The output of the model $y = \\sigma(z)$ can be interpreted as a probability $y$ that input $z$ belongs to one class $(t=1)$, or probability $1-y$ that $z$ belongs to the other class $(t=0)$ in a two class classification problem. We note this down as: $P(t=1| z) = \\sigma(z) = y$.\n",
    "\n",
    "The neural network model will be optimized by maximizing the [likelihood](http://en.wikipedia.org/wiki/Likelihood_function) that a given set of parameters $\\theta$ of the model can result in prediction of the correct class of each input sample. The parameters $\\theta$ transform each input sample $i$ into an input to the logistic function $z_{i}$. The likelihood maximization can be written as:\n",
    "\n",
    "$$\\underset{\\theta}{\\text{argmax}}\\; \\mathcal{L}(\\theta|t,z) = \\underset{\\theta}{\\text{argmax}} \\prod_{i=1}^{n} \\mathcal{L}(\\theta|t_i,z_i)$$\n",
    "\n",
    "The likelihood $\\mathcal{L}(\\theta|t,z)$ can be rewritten as the [joint probability](http://en.wikipedia.org/wiki/Joint_probability_distribution) of generating $t$ and $z$ given the parameters $\\theta$: $P(t,z|\\theta)$. Since $P(A,B) = P(A|B)*P(B)$ this can be written as:\n",
    "\n",
    "$$P(t,z|\\theta) = P(t|z;\\theta)P(z|\\theta)$$\n",
    "\n",
    "Since we are not interested in the probability of $z$ we can reduce this to: $\\mathcal{L}(\\theta|t,z) = P(t|z;\\theta) = \\prod_{i=1}^{n} P(t_i|z_i;\\theta)$. \n",
    "Since $t_i$ is a [Bernoulli variable](http://en.wikipedia.org/wiki/Bernoulli_distribution), and the probability $P(t| z) = y$ is fixed for a given $\\theta$ we can rewrite this as: \n",
    "\n",
    "$$\\begin{split}\n",
    "P(t|z) & = \\prod_{i=1}^{n} P(t_i=1|z_i)^{t_i} * (1 - P(t_i=1|z_i))^{1-t_i} \\\\\n",
    "& = \\prod_{i=1}^{n} y_i^{t_i} * (1 - y_i)^{1-t_i} \\end{split}$$\n",
    "\n",
    "Since the logartimic function is a monotone increasing function we can optimize the log-likelihood function $\\underset{\\theta}{\\text{argmax}}\\; log \\mathcal{L}(\\theta|t,z)$. This maximum will be the same as the maximum from the regular likelihood function. The log-likelihood function can be written as:\n",
    "\n",
    "$$\\begin{split} log \\mathcal{L}(\\theta|t,z) & = log \\prod_{i=1}^{n} y_i^{t_i} * (1 - y_i)^{1-t_i} \\\\\n",
    "& = \\sum_{i=1}^{n} t_i log(y_i) + (1-t_i) log(1 - y_i)\n",
    "\\end{split}$$\n",
    "\n",
    "Minimizing the negative of this function (minimizing the negative log likelihood) corresponds to maximizing the likelihood. This error function $\\xi(t,y)$ is typically known as the [cross-entropy error function](http://en.wikipedia.org/wiki/Cross_entropy) (also known as log-loss):\n",
    "\n",
    "$$\\begin{split}\n",
    "\\xi(t,y) & = - log \\mathcal{L}(\\theta|t,z) \\\\\n",
    "& = - \\sum_{i=1}^{n} \\left[ t_i log(y_i) + (1-t_i)log(1-y_i) \\right] \\\\\n",
    "& = - \\sum_{i=1}^{n} \\left[ t_i log(\\sigma(z) + (1-t_i)log(1-\\sigma(z) \\right]\n",
    "\\end{split}$$\n",
    "\n",
    "This function looks complicated but besides the previous derivation there are a couple of intuitions why this function is used as a cost function for logistic regression. First of all it can be rewritten as:\n",
    "\n",
    "$$ \\xi(t_i,y_i) = \n",
    "   \\begin{cases}\n",
    "   -log(y_i) & \\text{if } t_i = 1 \\\\\n",
    "   -log(1-y_i) & \\text{if } t_i = 0\n",
    "  \\end{cases}$$\n",
    "  \n",
    "Which in the case of $t_i=1$ is $0$ if $y_i=1$ $(-log(1)=0)$ and goes to infinity as $y_i \\rightarrow 0$ $(\\underset{y \\rightarrow 0}{\\text{lim}}  -log(y) = +\\infty)$. The reverse effect is happening if $t_i=0$.  \n",
    "So what we end up with is a cost function that is $0$ if the probability to predict the correct class is $1$, and goes to infinity as the probability to predict the correct class goes to $0$.\n",
    "\n",
    "Notice that the cost function $\\xi(t,y)$ is equal to the negative [log probability](http://en.wikipedia.org/wiki/Log_probability) that $z$ is classified as its correct class:  \n",
    "$-log(P(t=1| z)) = -log(y)$,  \n",
    "$-log(P(t=0| z)) = -log(1-y)$.\n",
    "\n",
    "By minimizing the negative log probability we will maximize the log probability. And since $t$ can only be $0$ or $1$, we can write $\\xi(t,y)$ as:\n",
    "$$ \\xi(t,y) = -t log(y) - (1-t)log(1-y) $$\n",
    "\n",
    "Which will give $\\xi(t,y) = - \\sum_{i=1}^{n} \\left[ t_i log(y_i) + (1-t_i)log(1-y_i) \\right]$ if we sum over all $n$ samples.\n",
    "\n",
    "\n",
    "Another reason to use the cross-entropy function is that in simple logistic regression this results in a [convex](http://en.wikipedia.org/wiki/Convex_function) cost function, of which the global minimum will be easy to find. Note that this is not necessaraly the case anymore in multilayer neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivative of the cross-entropy cost function for the logistic function\n",
    "\n",
    "The derivative ${\\partial \\xi}/{\\partial y}$ of the cost function with respect to its input can be calculated as:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\frac{\\partial \\xi}{\\partial y} & = \\frac{\\partial (-t log(y) - (1-t)log(1-y))}{\\partial y} = \\frac{\\partial (-t log(y))}{\\partial y} +  \\frac{\\partial (- (1-t)log(1-y))}{\\partial y} \\\\\n",
    "& = -\\frac{t}{y} + \\frac{1-t}{1-y} = \\frac{y-t}{y(1-y)}\n",
    "\\end{split}$$\n",
    "\n",
    "This derivative will give a nice formula if it is used to calculate the derivative of the cost function with respect to the inputs of the classifier ${\\partial \\xi}/{\\partial z}$ since the derivative of the logistic function is ${\\partial y}/{\\partial z} =  y (1-y)$:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial z} = \\frac{\\partial y}{\\partial z} \\frac{\\partial \\xi}{\\partial y} = y (1-y) \\frac{y-t}{y(1-y)} = y-t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "\n",
    "\n",
    "The [logistic output function](http://en.wikipedia.org/wiki/Logistic_function) described above can only be used for the classification between two target classes $t=1$ and $t=0$. This logistic function can be generalized to output a multiclass categorical probability distribution by the [softmax function](http://en.wikipedia.org/wiki/Softmax_function). This softmax function $\\varsigma$ takes as input a $C$-dimensional vector $\\mathbf{z}$ and outputs a $C$-dimensional vector $\\mathbf{y}$ of real values between $0$ and $1$. This function is a normalized exponential and is defined as:\n",
    "\n",
    "$$ y_c = \\varsigma(\\mathbf{z})_c = \\frac{e^{z_c}}{\\sum_{d=1}^C e^{z_d}} \\quad \\text{for} \\; c = 1 \\cdots C$$\n",
    "\n",
    "The output $\\sum_{d=1}^C e^{z_d}$ acts as a regularizer to make sure that $\\sum_{c=1}^C y_c = 1$. \n",
    "As the output layer of a neural network the softmax function can be represented graphically as a layer with $C$ neurons.\n",
    "\n",
    "We can write the probabilities that the class is $t=c$ for $c = 1 \\ldots C$ given input $\\mathbf{z}$ as:\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} \n",
    "P(t=1 | \\mathbf{z}) \\\\\n",
    "\\vdots \\\\\n",
    "P(t=C | \\mathbf{z}) \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix} \n",
    "\\varsigma(\\mathbf{z})_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\varsigma(\\mathbf{z})_C \\\\\n",
    "\\end{bmatrix}\n",
    "= \\frac{1}{\\sum_{d=1}^C e^{z_d}}\n",
    "\\begin{bmatrix} \n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_C} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $P(t=c | \\mathbf{z})$ is thus the probability that that the class is $c$ given the input $\\mathbf{z}$.\n",
    "\n",
    "These probabilities of the output $P(t=1|\\mathbf{z})$ for an example system with 2 classes ($t=1$, $t=2$) and input $\\mathbf{z} = [z_1, z_2]$ is shown in the figure below. The other probability $P(t=2|\\mathbf{z})$ will be complementary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the softmax function\n",
    "def softmax(z): return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "# Plot the softmax output for 2 dimensions for both classes\n",
    "# Plot the output in function of the weights\n",
    "# Define a vector of weights for which we want to plot the ooutput\n",
    "nb_of_zs = 200\n",
    "zs = np.linspace(-10, 10, num=nb_of_zs) # input \n",
    "zs_1, zs_2 = np.meshgrid(zs, zs) # generate grid\n",
    "y = np.zeros((nb_of_zs, nb_of_zs, 2)) # initialize output\n",
    "# Fill the output matrix for each combination of input z's\n",
    "for i in xrange(nb_of_zs):\n",
    "    for j in xrange(nb_of_zs):\n",
    "        y[i,j,:] = softmax(np.asarray([zs_1[i,j], zs_2[i,j]]))\n",
    "# Plot the cost function surfaces for both classes\n",
    "fig = plt.figure()\n",
    "# Plot the cost function surface for t=1\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(zs_1, zs_2, y[:,:,0], linewidth=0, cmap=cm.coolwarm)\n",
    "ax.view_init(elev=30, azim=70)\n",
    "fig.colorbar(surf)\n",
    "ax.set_xlabel('$z_1$', fontsize=15)\n",
    "ax.set_ylabel('$z_2$', fontsize=15)\n",
    "ax.set_zlabel('$y_1$', fontsize=15)\n",
    "ax.set_title ('$P(t=1|\\mathbf{z})$')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of the softmax function\n",
    "\n",
    "To use the softmax function in neural networks we need to compute its derivative. This derivative ${\\partial y_i}/{\\partial z_j}$ of the output $\\mathbf{y}$ of the softmax function with respect to its input $\\mathbf{z}$ can be calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{if} \\; i = j :& \\frac{\\partial y_i}{\\partial z_i} = \\frac{\\partial \\frac{e^{z_i}}{e^{z_i}}}{\\partial z_i} = \\frac{e^{2z_i}-e^{2z_i}}{e^{2z_i}} = \\frac{e^{z_i}}{e^{z_i}} \\frac{e^{z_i}-e^{z_i}}{e^{z_i}} = \\frac{e^{z_i}}{e^{z_i}} (1 - \\frac{e^{z_i}}{e^{z_i}}) = y_i (1 - y_i) \\\\\n",
    "\\text{if} \\; i \\neq j :& \\frac{\\partial y_i}{\\partial z_j} = \\frac{\\partial \\frac{e^{z_i}}{e^{z_j}}}{\\partial z_j} = \\frac{0 - e^{z_i}e^{z_j}}{e^{2z_j}} = -\\frac{e^{z_i}}{e^{z_j}} \\frac{e^{z_j}}{e^{z_j}} = -y_i y_j\n",
    "\\end{split}$$\n",
    "\n",
    "This can be written in one formula with the help of the [Kronecker delta](http://en.wikipedia.org/wiki/Kronecker_delta) $\\delta_{ij}$ as:\n",
    "\n",
    "$$ \\frac{\\partial y_i}{\\partial z_j} = y_i (\\delta_{ij} - y_j) $$\n",
    "\n",
    "Note that if $i = j$ this derivative is similar to the derivative of the logistic function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy cost function for the softmax function\n",
    "\n",
    "To derive the cost function for the softmax function we start out from the [likelihood function](http://en.wikipedia.org/wiki/Likelihood_function) that a given set of parameters $\\theta$ of the model can result in prediction of the correct class of each input sample, as in the derivation for the logistic cost function. The maximization of this likelihood can be written as:\n",
    "\n",
    "$$\\underset{\\theta}{\\text{argmax}}\\; \\mathcal{L}(\\theta|\\mathbf{t},\\mathbf{z})$$\n",
    "\n",
    "The likelihood $\\mathcal{L}(\\theta|\\mathbf{t},\\mathbf{z})$ can be rewritten as the [joint probability](http://en.wikipedia.org/wiki/Joint_probability_distribution) of generating $\\mathbf{t}$ and $\\mathbf{z}$ given the parameters $\\theta$: $P(\\mathbf{t},\\mathbf{z}|\\theta)$. Which can be written as a conditional distribution:\n",
    "\n",
    "$$P(\\mathbf{t},\\mathbf{z}|\\theta) = P(\\mathbf{t}|\\mathbf{z};\\theta)P(\\mathbf{z}|\\theta)$$\n",
    "\n",
    "Since we are not interested in the probability of $\\mathbf{z}$ we can reduce this to: $\\mathcal{L}(\\theta|\\mathbf{t},\\mathbf{z}) = P(\\mathbf{t}|\\mathbf{z};\\theta)$. Which can be written as $P(\\mathbf{t}|\\mathbf{z})$ for fixed $\\theta$.\n",
    "Since each $t_i$ is dependent on the full $\\mathbf{z}$, and only 1 class can be activated in the $t$ we can write\n",
    "\n",
    "$$P(\\mathbf{t}|\\mathbf{z}) = \\prod_{i=c}^{C} P(t_c|\\mathbf{z})^{t_c} = \\prod_{i=c}^{C} \\varsigma(\\mathbf{z})_c^{t_c} = \\prod_{i=c}^{C} y_c^{t_c}$$\n",
    "\n",
    "\n",
    "As was noted during the derivation of the cost function of the logistic function, maximizing this likelihood can also be done by minimizing the negative log-likelihood:\n",
    "\n",
    "$$- log \\mathcal{L}(\\theta|\\mathbf{t},\\mathbf{z}) = \\xi(\\mathbf{t},\\mathbf{z}) = - log \\prod_{i=c}^{C} y_c^{t_c} = - \\sum_{i=c}^{C} t_c \\cdot log(y_c)$$\n",
    "\n",
    "Which is the cross-entropy error funtion $\\xi$.\n",
    "Note that for a 2 class system output $t_2 = 1 - t_1$ and this results in the same error function as for logistic regression: $\\xi(\\mathbf{t},\\mathbf{y}) =- t_c log(y_c) - (1-t_c) log(1-y_c) $.\n",
    "\n",
    "The cross-entropy error function over a batch of multiple samples of size $n$ can be calculated as:\n",
    "\n",
    "$\\xi(T,Y) = \\sum_{i=1}^n \\xi(\\mathbf{t}_i,\\mathbf{y}_i) = - \\sum_{i=1}^n \\sum_{i=c}^{C} t_{ic} \\cdot log( y_{ic}) $\n",
    "\n",
    "Where $t_{ic}$ is 1 if and only if sample $i$ belongs to class $c$, and $y_{ic}$ is the output probability that sample $i$ belongs to class $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivative of the cross-entropy cost function for the softmax function\n",
    "\n",
    "The derivative ${\\partial \\xi}/{\\partial z_i}$ of the cost function with respect to the softmax input $z_i$ can be calculated as:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\frac{\\partial \\xi}{\\partial z_i} & = - \\sum_{j=1}^C \\frac{\\partial t_j log(y_j)}{\\partial z_i}{} =\n",
    "- \\sum_{j=1}^C t_j \\frac{\\partial log(y_j)}{\\partial z_i} = - \\sum_{j=1}^C t_j \\frac{1}{y_j} \\frac{\\partial y_j}{\\partial z_i} \\\\\n",
    "& = - \\frac{t_i}{y_i} \\frac{\\partial y_i}{\\partial z_i} - \\sum_{j \\neq i}^C \\frac{t_j}{y_j} \\frac{\\partial y_j}{\\partial z_i}\n",
    "= - \\frac{t_i}{y_i} y_i (1-y_i) - \\sum_{j \\neq i}^C \\frac{t_j}{y_j} (-y_j y_i) \\\\\n",
    "& = - t_i + t_i y_i + \\sum_{j \\neq i}^C t_j y_i = - t_i + \\sum_{j = 1}^C t_j y_i\n",
    "= -t_i + y_i \\sum_{j = 1}^C t_j \\\\\n",
    "& = y_i - t_i\n",
    "\\end{split}$$\n",
    "\n",
    "Note that we already derived ${\\partial y_j}/{\\partial z_i}$ for $i=j$ and $i \\neq j$ above.\n",
    "\n",
    "The result that ${\\partial \\xi}/{\\partial z_i} = y_i - t_i$ for all $i \\in C$ is the same as the derivative of the cross-entropy for the logistic function which had only one output node."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
